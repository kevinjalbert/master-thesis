% !TEX root = thesis.tex
\chapter{Summary and Conclusions}
\label{chap:conclusions}


\section{Summary}
\label{sec:conclusions_summary}
Mutation testing is a resource intensive process, potentially producing thousands of mutants for a given software system. Recall that mutation testing generates a set of mutants from the source code of the \gls{sut} and than evaluates these using the provided test suite. A mutation score is calculated as a result of mutation testing, which indicates how effective the test suite is at finding faults (i.e., test suite effectiveness).

There have been a number of research studies aimed at improving mutation testing performance by adjusting the mutation testing process (i.e., better mutation representation/generation/evaluation), instead we applied machine learning to predict the mutation score of source code units. As described in Chapter~\ref{chap:approach} we use a \gls{svm} to make predictions based on the features of class- and method-level source code units. We use the source code and test suite as the source of metrics for our predictions as they are directly involved in the mutation testing process. Specifically, we identify four initial sets of metrics (i.e., feature sets) from the \gls{sut}: source code, coverage, accumulated source code, and accumulated test suite.

We evaluated our approach using eight test subjects that contained 1689 classes, 113686 method and 10233 test cases. Using the available data, we proposed five research questions in Section~\ref{sec:experiment_results}. The following summarizes our findings:

\begin{itemize}
  \item In Section~\ref{subsec:experiment_mutation_score_distribution} we determined that the eight test subjects had effective test suites, all but one exceeding a 73\% mutation score. By considering the distribution of mutation scores for our test subjects, we were able to determine three suitable ranges for mutation score categories to abstract the real-values of mutation scores.
  \item In Section~\ref{subsec:experiment_cross_validation} we used the available data and evaluated the different feature sets over all our test subjects. Using all feature sets provided the greatest cross-validation accuracy, outperforming the feature sets individually.
  \item In Section~\ref{subsec:experiment_prediction} we evaluated our prediction approach on unknown data within individual test subjects and across test subjects. Our results showed that class-level mutation score was more difficult to predict, while method-level predictions performed better than class-level predictions. Furthermore, method-level predictions of unknown data within an individual test subject yielded higher accuracy than across test subjects.
  \item In Section~\ref{subsec:experiment_optimization_generalization} we explored avenues to optimize and generalize our prediction approach. We identified that using cross-validation accuracy for \gls{svm} parameter selection can be ineffective. We presented several alternative and more effective performance measures, namely F-score. We performed our own grid search to identify a single set of \gls{svm} parameters that maximized F-score across all data sets. We identified generalizable \gls{svm} parameters for class- and method-level predictions, and re-evaluated our prediction accuracy using the new parameters. As a result we were able to increase the average prediction accuracy for class- and method-level source code units by +5.0\% and +3.6\% within systems, and +4.6\% and +5.2\% across systems for class- and method-level source code units.
  \item In Section~\ref{subsec:experiment_data} we conducted an experiment that observed how our approach's prediction accuracy changed with the availability of data. By limiting the amount of training data used for training we showed that a learning curve was clearly defined in some of our test subjects while not as pronounced in others. This experiment demonstrates that it is possible to train on a fraction of the available data and predict the remainder with near optimal accuracy. We showed the applicability of using our approach for an iterative development environment where it is possible to use mutation testing on a fraction of the available data and predict on the remaining amount.
\end{itemize}


\section{Contributions}
\label{sec:conclusions_contributions}
Based on our empirical evaluation (see Chapter~\ref{chap:experiment}) we produced the following contributions to the domain of mutation score prediction:

\begin{itemize}
  \item Proposed a new approach for predicting the mutation score of a class- and method-level source code unit using source code and test suite metrics. We performed an empirical evaluation of our approach over eight open source test subjects.
  \item Identified 33 specific metrics (further grouped into four logical sets) that characterize source code units with respect to mutation score predictions.
  \item Demonstrated through cross-validation that the four feature sets in combination provided more accuracy than that of the feature sets individually.
  \item Demonstrated that prediction on unknown data is possible. Predictions within an individual software system has more variation in the mutation score performance than predictions on an unknown software system. Specifically, we showed that using all the available features we achieved an average prediction accuracy within systems of 36.7\% and 56.8\%, for class- and method-level source code units. We also achieved an average prediction accuracy across systems of 39.0\% and 42.8\% for class- and method-level source code units.
  \item Identified a generalizable set of \gls{svm} parameters that maximized F-score over our test subjects. These parameters increased prediction accuracy (+5.0\% for class-level within systems, +4.6\% for class-level across systems, +3.6\% for method-level within systems, and +5.2\% for method-level across systems) over that was determined through cross-validation. We achieved higher than random prediction accuracies using generalized \gls{svm} parameters, which removes the need of finding suitable parameters for new data.
  \item Demonstrated that it is not necessary to train on 90\% of the available data (as in $10$-fold cross-validation) in order achieve near optimal prediction accuracy.
\end{itemize}

In summary, our approach is novel in that we considered both source code and test suite metrics as factors to make mutation score predictions. We also performed feature selection on the collected features in Appendix~\ref{app:feature_selection}, results indicated that it is possible to reduce the feature set and retain prediction accuracy.


\section{Limitations}
\label{sec:conclusions_limitations}
Several limitations of our approach and empirical evaluation include:

\begin{itemize}
  \item In our approach we removed abstract, overloaded and anonymous source code unit (due to their complexity), which reduced our usable data and could misrepresented the actual test subject's software systems.
  \item We calculated the \emph{NOT} (i.e., number of tests) metrics in our approach with \emph{Javalanche}, ideally this should be calculated using \emph{EMMA}.
  \item We used only eight open source test subjects, this is most likely not a representative sample of all software systems.
  \item We used only considered 33 possible source code and test suite metrics to predict mutation testing scores, mostly likely there are more metrics that could have been used for our predictions.
  \item We evaluated our approach using three categories to abstract the exact mutation score prediction. The results may not represent prediction on a finer grain set of mutation score categories.
\end{itemize}

Furthermore the issues concerning the threats to validity (see Section~\ref{sec:experiment_threats}) are also limitations to our approach and empirical evaluation.


\section{Future Work}
\label{sec:conclusions_future_work}
The future work for this thesis can be divided into two topics:

\begin{itemize}
  \item Improvement and optimization of our approach for predicting mutation scores (Section~\ref{subsec:conclusions_optimizing_approach}.
  \item Further experimentation with additional statistical measures to validate the generalizability of our results (Section~\ref{subsec:conclusions_experimental_evaluation}).
\end{itemize}

Furthermore, obviously we would like to address the limitations listed in the previous section.


\subsection{Optimizing and Improving Approach}
\label{subsec:conclusions_optimizing_approach}
In our approach we discard abstract/overloaded/anonymous source code units as they were a bit more complex to handle during the construction of our approach. As for future work it would be wise to reconsider these omitted details as they obviously contribute to the data (i.e., mainly for class-level source code units, which might explain why their prediction accuracy was lower). We would like to correctly determine the \emph{NOT} (i.e., number of tests) metric without relying on \emph{Javalanche} for coverage data. \emph{EMMA} is fully capable in determining the \emph{NOT} feature.

We would like to further explore additional metrics and other facets of a software system. For example, we would like to use the \emph{Software Testing and Reliability Early Warning (STREW) metric suite}~\cite{NWO+05, NWVO05} or even the number of assertions within test cases. Furthermore, we could explore the mutants themselves as the mutation generation is not the expensive aspect of mutation testing, or even runtime information. Negappan et al. mined metrics to predict component failures and stated that \emph{``Predictors are accurate only when obtained from the same or similar projects''}~\cite{NBZ06}. This suggests that prediction across software systems, as we did in for our empirical evaluation is not ideal. We would like to further investigate this with respect to our approach, and see if we can achieve higher prediction performance using similar projects while predicting across software systems.

By using \emph{Javalanche} we unfortunately did not have access to class-level object-oriented mutation operators, and a limited subset of traditional method-level mutation operators. Future work would be required to add these missing mutation operators into \emph{Javalanche} as it would not only benefit this thesis but also those that use \emph{Javalanche}. Furthermore, there exists more than the traditional mutation operators that generate typical faults, it would be a great addition to incorporate security and concurrency mutation operators into our approach. Equivalent mutants pose a challenge in interpreting mutation testing results, \emph{Javalanche} has an approach that attempts to mitigate the impact of equivalent mutants that we ignored. Further work can integrate this consideration into our approach, we initially did not include it as it would further reduce our available data.

There are standard optimizations that can be done for our implementation such as better data structures and taking advantage of concurrency. We also would like to adapt our approach so that others can use it from a usability point-of-view. For example, a simple script that allows a user to specify source code unit(s) to be predicted based on a already trained classification model or as an Eclipse plugin. As our approach uses a classification approach for prediction, it is possible to extract from the \gls{svm} the probability that a vector belongs to a specific category. We would like to take advantage of this and present this data as well as it illustrates the confidence in the predictions.


\subsection{Statistical and Experimental Evaluation}
\label{subsec:conclusions_experimental_evaluation}
With our experimental setup we utilize a minimum of ten executions to reduce the noise in our results. To further reinforce our results, future work would involve increasing the number of executions (i.e., between 25 and 100 executions). In our analysis of the results we primarily used summary statistics (i.e., mean, standard deviation, etc\ldots), while these statistics provide valid summary of the results there are other statistical measures that are stronger (i.e., confidence interval, hypothesis testing, etc\ldots). Furthermore we performed a number of experiments to evaluate our approach, these alone are not comprehensive in what could have been done. Additional analysis on the features and their relationship with each other and mutation scores would be an interesting study to conduct as it may provide additional detail on the source code units. Further investigation on the data is required to understand whether the predictive ability of our approach depends on the distribution/availably of data and/or the features used. There still remains a lot of experimentation to be done in this area with respect to our approach. For future work we would like to evaluate our current results and new experiments with stronger statistical measures.

With respect to our implementation we utilized a number of tools to gather features and the mutation scores. We would like to explore using other tools as alternative as this can show that our approach still functions correctly independently of the tools used. Due to our limited set of test subjects we did not have a wide variety of domains, source sizes, test suite sizes and mutation scores. By including more open source and potentially industrial software systems we can cover more pairings of the aforementioned criteria, which will shine insight on the generalizability of our approach.


\section{Conclusions}
\label{sec:conclusions_conclusions}
Mutation testing is just too costly, which inhibits industry adoption. We stated the following in Chapter~\ref{chap:introduction}:

\begin{quote}
  \emph{\textbf{Thesis Statement:} The use of source code and test suite metrics in combination with machine learning techniques can accurately predict mutation scores. Furthermore, the predictions can be used to reduce the performance cost of mutation testing when used to iteratively develop test suites.}
\end{quote}

We followed through with the thesis statement with the creation of such an approach to predict mutation scores using source code and test suite metrics. We discussed the necessary topics required for this thesis in Chapter~\ref{chap:background}. We described our approach along-side an example in Chapter~\ref{chap:approach}. We outlined a set of experiments in Chapter~\ref{chap:experiment} that evaluated our approach to answer several research questions related to our thesis statement. Finally we present limitations, threats to validity and future work in Chapter~\ref{chap:conclusions}.

With our approach we showed that it is indeed possible to predict mutation scores of source code units using source code and test suite metrics. For predictions on unknown source code units within a software system, we were able to achieve an average accuracy of 56.8\% for method-level predictions. Exploratory work on method-level prediction of unknown data across software systems provided lesser accuracy at 42.8\%. Both of these values are higher than random prediction accuracy (i.e., 33.3\%) using a general set of \gls{svm} parameters, which eases the complexity of tuning our technique. Class-level predictions did not fare as well compared to method-level predictions, with 36.7\% accuracy within systems and 39.0\% accuracy across systems. Contrary to other prediction techniques (i.e., bug detection) we observed the test suite in addition to the source code, which is quite novel. With future work we hope that test suite metrics can be further used in existing and future research. Furthermore we anticipate that our approach still has room for improvement with respect to generalizability and prediction accuracy.
