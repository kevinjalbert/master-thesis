% !TEX root = thesis.tex
\chapter{Empirical Evaluation}
\label{chap:experiment}
In this chapter we evaluate our approach from Chapter~\ref{chap:approach}. We describe how we setup and conduct our empirical evaluation in Section~\ref{sec:experiment_setup} and we describe our experimental method in Section~\ref{sec:experiment_method}. Finally we discuss our experimental results in Section~\ref{sec:experiment_results} and threats to validity in Section~\ref{sec:experiment_threats}.


\section{Experimental Setup}
\label{sec:experiment_setup}
To encourage reproducibility of our empirical evaluation we discuss the specific details concerning environment (Section~\ref{subsec:experiment_environment}), tool configuration (Section~\ref{subsec:experiment_tool_configuration}), test subjects (Section~\ref{subsec:experiment_test_subjects}), and data preprocessing (Section~\ref{subsec:experiment_data_preprocessing}).


\subsection{Tool Configuration}
\label{subsec:experiment_tool_configuration}
We use three tools in our approach -- \emph{Javalanche}, \emph{Eclipse Metrics Plugin} and \emph{EMMA}. For all three tools, our approach manipulate the raw output of these tools to better support data synthesis. We use the default configuration for \emph{Eclipse Metrics Plugin} and the \emph{EMMA} as these are already configured to provide the necessary data.

We configured \emph{Javalanche} to better suite our approach. Although \emph{Javalanche} has the ability to run parallel tasks, we did not utilize this feature. This was to avoid unnecessary issues that can occur due to concurrent access to file resources that a test suite may use. We enabled the coverage impact analysis of \emph{Javalanche} as it provides comprehensive data regarding the mutants such as the type, location, and whether or not it was killed. The additional analysis is useful and required in the implementation of our approach, though it reduces the performance of \emph{Javalanche}.

We perform $10$-fold cross-validation as described in Section~\ref{sec:background_machine_learning}. \emph{LIBSVM} provides an \emph{easy script} that automatically scales the data and makes parameter selection using a grid search~\cite{HCL03}. A grid search iterates over a range of parameters while measuring the effect it has on the classifiers performance. Parameter selection is a critical aspect of machine learning algorithms, and it can greatly influence the classification accuracy. We allow \emph{LIBSVM} to automatically take care of this to best select the parameters based on the provided data. We allow \emph{LIBSVM} to use eight threads for computation tasks. We also utilize the radial basis function kernel as it is the default and comes recommended by the authors~\cite{HCL03}.

\subsection{Test Subjects}
\label{subsec:experiment_test_subjects}
We constructed three simple criteria to select our test subjects:

\begin{itemize}
  \item We selected software system that have a minimum of 5000 total \gls{sloc}. We decided to use this size as our minimum to avoid selecting \emph{toy} software systems that are not similar to \emph{real} software systems. Also, by using \emph{real} software systems we can potentially collect more data than that of \emph{toy} software systems.
  \item We selected open source projects as they are relatively easy to acquire as opposed to industry projects, and are freely available to analyze.
  \item Our approach required projects with a test suite or set of test cases as it is fundamentally required for mutation testing.
\end{itemize}

\begin{sidewaystable}[!tb]
  \centering
  \caption{The set of test subjects along with source code and test suite metrics.}
  \label{tab:experimental_subjects}
  \rowcolors{2}{gray!30}{gray!20}
  \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{1.9cm}|>{\raggedleft\arraybackslash}p{1.9cm}|>{\raggedleft\arraybackslash}p{1.9cm}|>{\raggedleft\arraybackslash}p{1.9cm}|>{\raggedleft\arraybackslash}p{1.9cm}|>{\raggedleft\arraybackslash}p{1.9cm}|>{\raggedleft\arraybackslash}p{1.9cm}|}
    \hline
    \rowcolor[RGB]{169,196,223}
    \textbf{Test Subject} & \textbf{Source LOC} & \textbf{Source Classes} & \textbf{Source Methods} & \textbf{Test LOC} & \textbf{Test Classes} & \textbf{Test Methods} & \textbf{Test Cases} \\
    \hline \emph{logback-core} (1.0.3)~\cite{logback} & 12118 & 249 & 1270 & 8377 & 174 & 688 & 286 \\
    \hline \emph{barbecue} (1.5-beta1)~\cite{barbecue} & 4790 & 58 & 299 & 2910 & 38 & 416 & 225 \\
    \hline \emph{jgap }(3.6.1)~\cite{jgap} & 28975 & 415 & 3017 & 19694 & 180 & 1633 & 1355 \\
    \hline \emph{commons-lang} (3.1)~\cite{commons-lang} & 19499 & 149 & 1196 & 33332 & 242 & 2408 & 2050 \\
    \hline \emph{joda-time} (2.0)~\cite{joda-time} & 27139 & 227 & 3635 & 51388 & 221 & 4755 & 3866 \\
    \hline \emph{openfast} (1.1.0)~\cite{openfast} & 11646 & 265 & 1447 & 5587 & 115 & 421 & 322 \\
    \hline \emph{jsoup} (1.6.2)~\cite{jsoup} & 10949 & 198 & 954 & 2883 & 25 & 335 & 319 \\
    \hline \emph{joda-primitives} (1.0)~\cite{joda-primitives} & 11157 & 128 & 1868 & 6989 & 49 & 746 & 1810 \\
    \hline \textbf{all} & \textbf{126273} & \textbf{1689} & \textbf{13686} & \textbf{131160} & \textbf{1044} & \textbf{11402} & \textbf{10233} \\
    \hline
  \end{tabular}
\end{sidewaystable}
\afterpage\clearpage

Following the criteria outlined, we selected the following eight open source Java software systems shown in Table~\ref{tab:experimental_subjects}. A brief description of each open source software system used in our empirical evaluation is presented as follows:

\begin{itemize}
  \item \textbf{\emph{logback-core}}: \emph{``Logback is intended as a successor to the popular log4j project, picking up where log4j leaves off. The logback-core module lays the groundwork for the other two modules''}~\cite{logback}.
  \item \textbf{\emph{barbecue}}: \emph{``Barbecue is an open source, Java library that provides the means to create barcodes for printing and display in Java applications''}~\cite{barbecue}.
  \item \textbf{\emph{jgap}}: \emph{``JGAP is a Genetic Algorithms and Genetic Programming component provided as a Java framework''}~\cite{jgap}.
  \item \textbf{\emph{commons-lang}}: \emph{``The standard Java libraries fail to provide enough methods for manipulation of its core classes. Apache Commons Lang provides these extra methods''}\cite{commons-lang}.
  \item \textbf{\emph{joda-time}}: \emph{``Joda-Time provides a quality replacement for the Java date and time classes. The design allows for multiple calendar systems, while still providing a simple API''}~\cite{joda-time}.
  \item \textbf{\emph{openfast}}: \emph{``OpenFAST is a 100\% Java implementation of the FAST Protocol (FIX Adapted for STreaming). The FAST protocol is used to optimize communications in the electronic exchange of financial data''}~\cite{openfast}.
  \item \textbf{\emph{jsoup}}: \emph{``jsoup is a Java library for working with real-world HTML. It provides a very convenient API for extracting and manipulating data, using the best of DOM, CSS, and jquery-like methods''}~\cite{jsoup}.
  \item \textbf{\emph{joda-primitives}}: \emph{``Joda Primitives provides collections and utilities to bridge the gap between objects and primitive types in Java''}~\cite{joda-primitives}.
\end{itemize}

For our experiments we have eight test subjects that we can use individually, as well as consider them collectively. Therefore, we further refer to the collective set of all the individual test subjects as the \emph{all} subject. By combining the individual test subjects we can evaluate our approach on a mixed set of data. Though each test subject is unique in terms of the functionality they provide and the specific structural design choices, each one is shares the commonality of being a software system. To further explore how our approach performs on different data, we consider eight additional sets using the \emph{all} subject as the base, excluding an individual test subject. In other words we have eight \emph{all\_but\_<subject>} subjects, which is a combination of each individual test subject except the \emph{<subject>}. These additional subjects allow us to evaluate our prediction approach by keeping one test subject completely isolated from the rest. As our approach focuses on prediction of mutation scores using metrics of the test subjects, the \emph{all} and \emph{all\_but\_<subject>} subjects allow us to evaluate the generalizability of our approach on mixed and isolated test subjects.


\subsection{Data Preprocessing}
\label{subsec:experiment_data_preprocessing}
We first run each test subject through a verification \emph{test} that \emph{Javalanche} provides. This test identifies any unit test cases that cannot execute properly or fail within \emph{Javalanche}. We have to remove these test cases as the mutation testing process requires a test suite with no errors. We then import all the test subjects into Eclipse, as that is where the \emph{Eclipse Metrics Plugin} operates. With our approach we simply configure our scripts to identify the test subject to collect data from, and the results are then stored in a database.


\subsection{Environment}
\label{subsec:experiment_environment}
We conducted all of the experiments for our empirical evaluation on a single machine that has six GB of random access memory, a hard drive disk running at 7200 revolutions per minute  and an Intel Core i7-870 processor running at 2.93 gigahertz. The environment is relevant as the mutation testing performance cost is dependant on the processor and hard drive disk speed.


\section{Experimental Method}
\label{sec:experiment_method}
Our empirical evaluation consists of five separate experiments:

\begin{itemize}
  \item \textbf{Mutation Score Distribution (Section~\ref{subsec:experiment_mutation_score_distribution})}: Using our test subjects described in Section~\ref{subsec:experiment_test_subjects}, we first want to understand their mutation testing results. Using our approach, we collect the source code, test suite metrics, and mutation scores of each test subject. As each test subject consists of multiple class- and method-level source code units, we are interested in the distribution of mutation scores. By understanding the distribution, we can gauge the available data that we will have for the later experiments and detect possible anomalies. We can also identify classification categories for the future experiments based on the mutation distribution as \gls{svm} have difficulty prediction real values (i.e., the mutation score).
  \item \textbf{Cross-Validation (Section~\ref{subsec:experiment_cross_validation})}: We identified features that describe source code units in Table~\ref{tab:metrics}. We grouped the features into sets (see Table~\ref{tab:feature_sets}) so we could evaluate how each set influences our classification performance. In this section we acquire the cross-validation accuracy for using the different feature sets over all the available data to identify the best feature set. We then evaluate the cross-validation accuracy over each individual test subject using the best set of features.
  \item \textbf{Prediction on Unknown Data (Section~\ref{subsec:experiment_prediction})}: Cross-validation accuracy provides a good indicator of classification performance, however it does not simulate realistic prediction on unknown data. In this section, we  control the training and testing data for our \gls{svm} to evaluate the prediction accuracy on unknown data. Using the \emph{all\_but\_<subject>} subjects, we can evaluate the prediction accuracy when dealing with a unknown test subject that is isolated from the training data. We also evaluate the prediction accuracy of unknown data within an individual test subject. Both of these predictions are on unknown data but they explore the performance differences for prediction within a test subject, and against a different test subject.
  \item \textbf{Optimization and Generalization (Section~\ref{subsec:experiment_optimization_generalization})}: It is not known prior to predicting on unknown data what parameter values to use for the \gls{svm}. In Section~\ref{subsec:experiment_prediction}, the parameters are selected based on what maximizes the cross-validation accuracy with hopes that these parameters generalizes to the unknown data. In this section, we identify the most appropriate parameters that generalize to unknown data prediction. We evaluate the implications of using the generalizable parameters with respect to their impact on predicting unknown data.
  \item \textbf{Impact of Training Data Availability on Prediction Accuracy (Section~\ref{subsec:experiment_data})}: Using the findings from the previous experiments, we explore the impact of data availability on prediction accuracy. We graph the prediction accuracies against the amount of training data used for prediction. This experiment evaluates the applicability of using our approach in iterative development where it is beneficial to avoid evaluating every mutant generated.
\end{itemize}


\section{Experimental Results}
\label{sec:experiment_results}
The following five sections present experiments used in our empirical evaluation for our approach. Each section starts with a general research question that is addressed throughout the section.


\subsection{Mutation Score Distribution}
\label{subsec:experiment_mutation_score_distribution}
\begin{quote}
  \emph{\textbf{Research Question \#1:} What is the mutation score distribution of our test subjects?}
\end{quote}

\begin{quote}
  \emph{\textbf{Research Question \#2:} Using the distribution of our test subjects' mutation scores can we identify three categories of mutation scores to predict?}
\end{quote}

\begin{sidewaystable}[!tb]
  \centering
  \caption{Mutation testing results of the test subjects from Table~\ref{tab:experimental_subjects}.}
  \label{tab:experiments_mutation_results}
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft}p{2.5cm}|>{\raggedleft}p{2.5cm}|>{\raggedleft}p{2.5cm}|>{\raggedleft}p{2.5cm}|>{\raggedleft}p{2.5cm}|>{\raggedleft\arraybackslash}p{2.5cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Mutants Generated} & \textbf{Mutants Covered} & \textbf{Coverage Percent (\%)} & \textbf{Mutants Killed} & \textbf{Mutation Score (\%)\tnote{a}} & \textbf{Time Taken (\emph{hh:mm:ss})} \\
      \hline \emph{logback-core} & 10682 & 7350 & 68.8 & 5400 & 73.5 & 01:49:10 \\
      \hline \emph{barbecue} & 27324 & 4339 & 15.9 & 2727 & 62.8 & 00:49:51 \\
      \hline \emph{jgap} & 31929 & 17903 & 56.1 & 13328 & 74.4 & 07:04:44 \\
      \hline \emph{commons-lang} & 45141 & 41761 & 92.5 & 33772 & 80.9 & 15:51:59 \\
      \hline \emph{joda-time} & 70594 & 58595 & 83.0 & 48545 & 82.8 & 31:55:50 \\
      \hline \emph{openfast} & 14910 & 8371 & 56.1 & 6869 & 82.1 & 01:34:38 \\
      \hline \emph{jsoup} & 14165 & 10540 & 74.4 & 8430 & 80.0 & 03:55:56 \\
      \hline \emph{joda-primitives} & 22269 & 17334 & 77.8 & 13499 & 77.9 & 01:24:33 \\
      \hline \textbf{all} & \textbf{237014} & \textbf{166193} & \textbf{70.1} & \textbf{132570} & \textbf{79.8} & \textbf{64:26:41} \\
      \hline
    \end{tabular}
    \begin{tablenotes}
      \item[a] Mutation score is calculated using the covered and killed mutants.
    \end{tablenotes}
  \end{threeparttable}
  
  \vspace{3em}

  \centering
  \caption{The usable number of source code unit data points gathered from the test subjects in Table~\ref{tab:experimental_subjects}.}
  \label{tab:experiments_collected_data}
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|r|r|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Class-Level} & \textbf{Method-Level} \\
      \hline \emph{logback-core} & 115 & 447 \\
      \hline \emph{barbecue} & 31 & 143 \\
      \hline \emph{jgap} & 124 & 655 \\
      \hline \emph{commons-lang} & 124 & 789 \\
      \hline \emph{joda-time} & 194 & 2019 \\
      \hline \emph{openfast} & 120 & 401 \\
      \hline \emph{jsoup} & 83 & 381 \\
      \hline \emph{joda-primitives} & 73 & 675 \\
      \hline \textbf{all} & \textbf{864} & \textbf{5510} \\
      \hline
    \end{tabular}
  \end{threeparttable}
\end{sidewaystable}
\afterpage\clearpage

\noindent
As described in Chapter~\ref{chap:approach}, our approach uses mutation testing to acquire the necessary mutation score data used for the category of the source code units. Table~\ref{tab:experiments_mutation_results} shows the results of running \emph{Javalanche} on our test subjects. For all of our test subjects, mutation testing produced a large number of mutants that were evaluated, taking approximately 64 hours to complete in our experimental environment. As described in Section~\ref{subsec:background_mutation_tools}, \emph{Javalanche} utilizes \emph{coverage} (i.e., basic block coverage) for test selection which limits the number of mutants to be evaluated to a subset of covered mutants. \emph{Javalanche} was able to kill 79.8\% of the covered mutants using all projects cumulatively. All individual projects, except for \emph{barbecue}, exceeded a mutation score of at least 73\% which indicates that the test suites for covered source code units are reasonably effective. The overall coverage is 70.1\%, indicating that the test suites of the projects did not cover the reminding 29.9\% of the generated mutants. Realistically, mutation scores are calculated using the entire project's source code, but for our purpose only covered mutants were used. The corrective action for non-covered mutants (i.e., mutants not covered by test suite using basic block coverage) is to add new test cases that provide coverage over the mutant's location.

\begin{figure}[!tb]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
      bar width=1,
      ymajorgrids=true,
      xlabel=Mutation Score (\%),
      ylabel=\# of Classes,
      width=\linewidth,
      height=9.0cm]
      \addplot[ybar,fill=black] file {plots/all/evaluation_projects_class_distribution.txt};
  \end{axis}
  \end{tikzpicture}
  \caption{Mutation score distribution of classes from all eight test subjects from Table~\ref{tab:experimental_subjects} that can be used for training.}
  \vspace{2mm}
  \hrule
  \label{fig:mutation_distributions_class_all}
\end{figure}

\begin{figure}[!tb]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
      bar width=1,
      ymajorgrids=true,
      xlabel=Mutation Score (\%),
      ylabel=\# of Methods,
      width=\linewidth,
      height=9.0cm]
      \addplot[ybar,fill=black] file {plots/all/evaluation_projects_method_distribution.txt};
  \end{axis}
  \end{tikzpicture}
  \caption{Mutation score distribution of methods from all eight test subjects from Table~\ref{tab:experimental_subjects} that can be used for training.}
  \vspace{2mm}
  \hrule
  \label{fig:mutation_distributions_method_all}
\end{figure}

\begin{figure}[!tb]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
      xtick={0, 25, 50, 75, 100, 125, 150, 175, 200},
      xticklabels={0, 25, 50, 75, 100, 125, 150, 175, 200..6507},
      bar width=1,
      ymajorgrids=true,
      xlabel=\# of Covered Mutants,
      ylabel=\# of Classes,
      width=\linewidth,
      height=9.0cm]
      \addplot[ybar,fill=black] file {plots/all/covered_mutants_class_distribution.txt};
  \end{axis}
  \end{tikzpicture}
  \caption{Covered mutant distribution of classes from all eight test subjects from Table~\ref{tab:experimental_subjects} that can be used for training.}
  \vspace{1mm}
  \footnotesize{\emph{The above figure presents a subset of the full distribution by collapsing the values that exceed 200 into a single data point. The max number of covered mutants found was 6507, which corresponds to the following class \texttt{org.joda.time.format.ISODateTimeFormat} from the \texttt{joda-time} project.}}
  \vspace{2mm}
  \hrule
  \label{fig:covered_mutant_distributions_class_all}
\end{figure}

\begin{figure}[!tb]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
      xtick={0, 25, 50, 75, 100, 125, 150, 175, 200},
      xticklabels={0, 25, 50, 75, 100, 125, 150, 175, 200..587},
      bar width=1,
      ymajorgrids=true,
      xlabel=\# of Covered Mutants,
      ylabel=\# of Methods,
      width=\linewidth,
      height=9.0cm]
      \addplot[ybar,fill=black] file {plots/all/covered_mutants_method_distribution.txt};
  \end{axis}
  \end{tikzpicture}
  \caption{Covered mutant distribution of methods from all eight test subjects from Table~\ref{tab:experimental_subjects} that can be used for training.}
  \vspace{1mm}
  \footnotesize{\emph{The above figure presents a subset of the full distribution by collapsing the values that exceed 200 into a single data point. There were 51 methods that had 117 covered mutants each, of these 48 are similar and are contained within the \texttt{ISODateTimeFormat} class of \texttt{joda-time}. The max number of covered mutants was 587, which corresponds to the \texttt{org.joda.time.format.PeriodFormatterBuilder\$FieldFormatter.parseInto} method.}}
  \vspace{2mm}
  \hrule
  \label{fig:covered_mutant_distributions_method_all}
\end{figure}

Source code and test code metrics were collected as described in Chapter~\ref{chap:approach}, which represented the set of feature data that make up the vectors of our \gls{svm}. Our approach can only make predictions using the synthesis of both mutation score data (i.e., category data) and source and test suite metrics (i.e., feature data) of source code units. If any piece of data is missing, we cannot use that source code unit for training and prediction purposes. Using our approach, we collected data for 864 class-level and 5510 method-level source code units (see Table~\ref{tab:experiments_collected_data}). We ignored abstract, anonymous, and overloaded source code units, because taking these into account would be a complex task. In addition to the ignored source code units we also ignored units with missing data (i.e., no tests cases), restricting the available data for further experiments. We present the distribution of the all the collected data points\footnote{\emph{Data points} are the \emph{vectors} or \emph{rows of data} within a \gls{svm} \emph{.libsvm} file.} for both class-level and method-level source code units with respect to mutation score in Figure~\ref{fig:mutation_distributions_class_all}~and~\ref{fig:mutation_distributions_method_all}. The mutation score distributions for each individual project are found in Appendix~\ref{app:mutation_score_distributions}. The mutation distribution of the both class- and method-level source code units are both negatively skewed, confirming our earlier observation that the test suite for the collected source code units are reasonably strong at detecting faults. We noticed that there were spikes comparative to their surroundings at the  0\%, 50\% and 100\% mutation score values, in particular the 100\% value is two to nine times greater then other areas respectively. We speculate the spikes occur because a large number of source code units probably have small number of covered mutants (i.e., easier to kill all, evenly kill half or kill none). Analysis of the covered mutant distribution for class-level source code units (as seen in Figure~\ref{fig:covered_mutant_distributions_method_all}) shows a slightly denser grouping for low covered mutants. The positively skewed distribution of covered mutants for method-level source code units supports our speculation. With respect to the percentile of the class-level distribution of covered mutants, a quarter of the classes have less then 16 covered mutants. The method-level results show that half of the method have less then six covered mutants, and a quarter of the methods have less then two covered mutants. The distributions of covered mutants confirm our speculation that many of the source code units have a low number of covered mutants, which can contribute to the high 0\%, 50\% and 100\% mutation scores.

In Section~\ref{subsec:approach_create_libsvm_file}, we mentioned that our approached would create \emph{.libsvm} files using the acquire data. The category data required for the files that the \gls{svm} utilized based on the mutation score of source code units. To avoid predicting the exact mutation score (i.e., a set of real numbers), we instead used an abstracted set of categories (i.e., ranges of mutation scores). We were unsure how to properly select the ranges to use for our categories, but eventually decided to base our categories on the distribution of mutation scores from Figure~\ref{fig:covered_mutant_distributions_class_all}~and~\ref{fig:covered_mutant_distributions_method_all}. We found that the class-level distribution has a 25$^{th}$, 50$^{th}$ and 75$^{th}$ percentile of 72\%, 81\%, 89\% respectively, and for same percentiles of the method-level distribution are 75\%, 87\%, 99\%. Using these values we decided to use the following as our general case for categories:

\begin{itemize}
  \item \textbf{LOW} = [0\% -- 70\%)
  \item \textbf{MEDIUM} = [70\% -- 90\%)
  \item \textbf{HIGH} = [90\% -- 100\%]
\end{itemize}

The rational behind the categories is to separate the lower and upper percentiles in to the LOW and HIGH category, with the remaining into the MEDIUM category. We believe that these values will provide a sufficient level of information over the mutation testing coverage of the source code units.


\subsection{Cross-Validation}
\label{subsec:experiment_cross_validation}
\begin{quote}
  \emph{\textbf{Research Question:} Using the test suite and source code data from our test subjects can we identify a set of features that maximize cross-validation accuracy?}
\end{quote}

\begin{table}[!tb]
  \centering
  \rowcolors{2}{gray!30}{gray!20}
  \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{3cm}|>{\raggedleft\arraybackslash}p{3cm}|>{\raggedleft\arraybackslash}p{3cm}|}
    \hline
    \rowcolor[RGB]{169,196,223}
    \textbf{Category} & \textbf{Mutation Score Range} & \textbf{Class-Level} & \textbf{Method-Level} \\
    \hline LOW & [0\% -- 70\%) & 191 & 1104 \\
    \hline MEDIUM & [70\% -- 90\%) & 459 & 1782 \\
    \hline HIGH & [90\% -- 100\%] & 214 & 2624 \\
    \hline
  \end{tabular}
  \caption{The available number of source code units that fall within the determined ranges of mutation scores.}
  \vspace{2mm}
  \hrule
  \label{tab:available_data}
\end{table}

\noindent
Using the determined classification categories from the previous section, we have imbalanced training data for class- and method-level source code units as shown in Table~\ref{tab:available_data}. Imbalanced data occurs when the data is not evenly distributed across the classification categories. This can be problematic for supervised machine learning techniques as they will heavily classify towards the majority category~\cite{BOSB10}. Barandela et al. indicate that there are three strategies to reduce the problem of imbalanced training data: Over-sample, under-sample, or internally bias the classification process~\cite{BVSF04}. It was shown that simple random under-sampling can be an effective solution (though not always the best) to this problem~\cite{Jap00,AKJ04}. As Akbani et al. mentioned \emph{``\ldots we are throwing away valid instances, which contain valuable information''}~\cite{AKJ04}, therefore we might be limiting the ability to generalize to new unknown data. The alternative is to perform over-sampling, as Batista et al. mention \emph{``\ldots random over-sampling can increase the likelihood of occurring overfitting, since it makes exact copies of the minority class examples''}~\cite{BPM04}. We decided to utilize random under-sampling as it provides a simple approach to dealing with imbalanced data. Furthermore the imbalanced data is not too severe (minority to majority ratio is approximately two to five), thus we believe we are not losing that much information by under-sampling our training data.

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={.95\textwidth}{.95\textheight}}
    \input{plots/all_cross_validation_features_class_graph}
  \end{adjustbox}
  \caption{Class-level cross-validation accuracy of feature sets on the \emph{all} subject.}
  \vspace{2mm}
  \hrule
  \label{fig:all_cross_validation_features_class_graph}
\end{figure}

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={.95\textwidth}{.95\textheight}}
    \input{plots/all_cross_validation_features_method_graph}
  \end{adjustbox}
  \caption{Method-level cross-validation accuracy of feature sets on the \emph{all} subject.}
  \vspace{2mm}
  \hrule
  \label{fig:all_cross_validation_features_method_graph}
\end{figure}

We use the \emph{LIBSVM}~\cite{CL11} library to perform $10$-fold cross-validation. To evaluate the cross-validation accuracy of the acquired data we randomly under-sample the data to balance the amount of data points within each category. We utilize 191 class-level and 1104 method-level source code units data points from each category, these values are chosen to maximize the number of data points from the minority category using random under-sampling.

Recall that we have a set of features (i.e., attributes for our vector in the \gls{svm}) as listed in Table~\ref{tab:feature_sets}. We explore the cross-validation accuracy using different feature sets (i.e., \ding{172}, \ding{173}, \ding{174}, \ding{175}) in Figures~\ref{fig:all_cross_validation_features_class_graph}~and~\ref{fig:all_cross_validation_features_method_graph}. The cross-validation accuracy of class- and method-level source code units were performed on the collective \emph{all} subject over ten executions to account for random undersampling of our data. To assess our cross-validation accuracy, we use random selection as our baseline. In our case, since we undersample our three categories, random selection will have an accuracy of 33.3\%. We can see that all feature sets are able to outperform random which indicates that there is some predictive power in the selected feature sets\footnote{Feature set \ding{174} for method-level source code units (Figure~\ref{fig:all_cross_validation_features_method_graph}) does not add any value (as it is specifically tailor for class-level source code units), and thus performs at random.}. We include a subset of all features (feature sets \ding{172} \ding{174} \ding{175}) to show the effects of merging only the source code and test suite metrics (excluding coverage feature set \ding{173}). We can clearly see that by using all feature sets together we can acquire higher cross-validation accuracy then using the feature sets alone. This observation supports our intuition that using various source code and test suite metrics together can predict the mutation scores of source code units well.

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={.95\textwidth}{.95\textheight}}
    \input{plots/individual_cross_validation_class_1_2_3_4_graph}
  \end{adjustbox}
  \caption{Class-level cross-validation accuracy of each test subject using all feature sets (\ding{172} \ding{173} \ding{174} \ding{175}).}
  \vspace{2mm}
  \hrule
  \label{fig:individual_cross_validation_class_1_2_3_4_graph}
\end{figure}

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={.95\textwidth}{.95\textheight}}
    \input{plots/individual_cross_validation_method_1_2_3_4_graph}
  \end{adjustbox}
  \caption{Method-level cross-validation accuracy of each test subject using all feature sets (\ding{172} \ding{173} \ding{174} \ding{175}).}
  \vspace{2mm}
  \hrule
  \label{fig:individual_cross_validation_method_1_2_3_4_graph}
\end{figure}

\begin{table}[!tb]
  \centering
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|r|r|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Class-Level} & \textbf{Method-Level} \\
      \hline \emph{logback-core} & 12 & 138 \\
      \hline \emph{barbecue} & 2 & 36 \\
      \hline \emph{jgap} & 27 & 197 \\
      \hline \emph{commons-lang} & 18 & 132 \\
      \hline \emph{joda-time} & 21 & 259 \\
      \hline \emph{openfast} & 24 & 73 \\
      \hline \emph{jsoup} & 13 & 58 \\
      \hline \emph{joda-primitives} & 1 & 165 \\
      \hline \emph{all} & 191 & 1104 \\
      \hline
    \end{tabular}
  \end{threeparttable}
  \caption{The number of data points used for each category based on undersampling the lowest category to provide balanced data, for each test subject.}
  \vspace{2mm}
  \hrule
  \label{tab:experiments_undersampled_data}
\end{table}

We have looked at the overall cross-validation accuracy using the different feature sets and found that using all feature sets provides the best accuracy. To understand how different projects behave when we apply our technique, we considered each test subject independently using all the feature sets. Figure~\ref{fig:individual_cross_validation_class_1_2_3_4_graph} illustrates the class-level cross-validations of each test subjects with the \emph{all} subject as a comparison. We can see that all but \emph{barbecue} and \emph{joda-primitives} are similar with respect to mean accuracy. All of the independent test subjects have larger variation in their accuracies compared to the method-level accuracies, which is most likely due to the limited data that each test subject provides on its own. Recall that we are undersampling our data to achieve balanced categories, thus in some situations the amount of data being used can drastically be reduced. In the case of \emph{barbecue}, the undersampling only allowed two instances of data to be used for each category, which explains the huge variation that it has. \emph{joda-primitives} only had one instance for each category, which resulted in a cross-validation accuracy of 0\%. Table~\ref{tab:experiments_undersampled_data} provides details regarding the number of data points being used with undersampling. Moving on to the method-level source code units presented in Figure~\ref{fig:individual_cross_validation_method_1_2_3_4_graph}, we can see that all but \emph{barbecue}, \emph{joda-primitives}, and \emph{joda-time} are comparable to the \emph{all} accuracy with slightly larger variations. Again, \emph{barbecue} has a low number of data points being used which can explain the larger variations in accuracy. With \emph{joda-primitives}, we have an unusually high cross-validation accuracy. If we look at Figure~\ref{fig:mutation_distributions_method_joda-primitives} in Appendix~\ref{app:mutation_score_distributions}, we can see that the mutation score distribution is cleanly separated according to the category ranges (i.e., a large number of 100\% and 50\% mutation score methods, with the remaining between these values). It might just be the case that the \emph{joda-primitives}'s data is easier to separate with the \gls{svm}, thus allowing it achieve higher accuracy. \emph{joda-time} presents a slightly higher cross-validation accuracy then the other test subjects. This could be because it has the most data points available for the \gls{svm} or because it has 48 methods that are very similar (most likely duplicates) as we saw in the covered mutant distribution (see Figure~\ref{fig:covered_mutant_distributions_method_all}). Similar methods would be classified in the same category, thus this could slightly inflate the cross-validation accuracy if this was the case.


\subsection{Prediction on Unknown Data}
\label{subsec:experiment_prediction}
\begin{quote}
  \emph{\textbf{Research Question \#1:} How well can our approach predict on unknown data, within an individual software system?}
  
  \emph{\textbf{Research Question \#2:} How well can our approach predict on unknown data, accross software systems?}
\end{quote}

\noindent
By using \emph{LIBSVM} we want to train a classifier such that it can predict well on \emph{unknown data}. Parameter selection and the training/testing data sets play an important role in developing a good classifier. Ultimately, we want to obtain a classifier that is able to \emph{generalize} to new, unknown data. In our specific case, we have eight different test subjects (that are most likely not similar to each other in terms of features) in which we want to maximize our performance at predicting unknown data. We used cross-validation in Section~\ref{subsec:experiment_cross_validation} as it mitigates the overfitting problem introduced by training (i.e., a model becomes specifically tuned for the training data set)~\cite{HCL03}. Parameter selection also occurred automatically using a grid search approach to find a set of parameters that maximized the cross-validation accuracy on the training data set.

\begin{table}[!tb]
  \centering
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{4.25cm}|>{\raggedleft\arraybackslash}p{4.25cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Class-Level\scriptsize\newline[LOW/MEDIUM/HIGH]} & \textbf{Method-Level\scriptsize\newline[LOW/MEDIUM/HIGH]} \\
      \hline \emph{logback-core} & 36/43/0 & 0/3/30 \\
      \hline \emph{barbecue} & 13/12/0 & 20/15/0 \\
      \hline \emph{jgap} & 24/19/0 & 26/0/38 \\
      \hline \emph{commons-lang} & 0/65/5 & 0/186/207 \\
      \hline \emph{joda-time} & 0/87/44 & 0/296/946 \\
      \hline \emph{openfast} & 0/33/15 & 0/69/113 \\
      \hline \emph{jsoup} & 0/18/26 & 0/50/157 \\
      \hline \emph{joda-primitives} & 0/64/6 & 0/105/75 \\
      \hline \emph{all\_but\_logback-core} & 48/55/12 & 138/141/168 \\
      \hline \emph{all\_but\_barbecue} & 15/14/2 & 56/51/36 \\
      \hline \emph{all\_but\_jgap} & 51/46/27 & 223/197/235 \\
      \hline \emph{all\_but\_commons-lang} & 18/83/23 & 132/318/339 \\
      \hline \emph{all\_but\_joda-time} & 21/108/65 & 256/555/1205 \\
      \hline \emph{all\_but\_openfast} & 24/57/39 & 73/142/186 \\
      \hline \emph{all\_but\_jsoup} & 13/31/39 & 58/108/215 \\
      \hline \emph{all\_but\_joda-primitives} & 1/65/7 & 165/270/240 \\
      \hline
    \end{tabular}
  \end{threeparttable}
  \caption{The number of data points present in each category for each test subject's prediction data set after undersampling (if possible) has occurred.}
  \vspace{2mm}
  \hrule
  \label{tab:experiments_remaining_data}
\end{table}

We conducted a number of tests where we use \emph{LIBSVM}'s \emph{easy script} to find the best parameters that maximize cross-validation accuracy and then apply the classifier to unknown data. We continue to undersample our data and conduct each experiment ten times to determine the prediction accuracy. We are interested in the prediction accuracy of unknown data within a system as well as across systems. See Table~\ref{tab:experiments_remaining_data} for the number of unknown data items being used for each subject with respect to predictions on unknown data. For within a system we train on the undersampled data of an individual test subject and predict on the remaining unknown data (i.e., what is left from the undersampling). For across systems we can consider the \emph{all\_but\_<subject>} subjects, where we train our classifier on all but the \emph{<subject>} and then predict on the excluded test subject. Section~\ref{subsubsec:prediction_within_system} presents results for prediction within a system, and Section~\ref{subsubsec:prediction_across_systems} presents results for prediction across systems.


\subsubsection{Prediction Within a System}
\label{subsubsec:prediction_within_system}
This experiment explores the capability of predicting unknown source code units within a software system. Situations such as the addition of new features or source code units fits this experiment.

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_class_within_graph}
  \end{adjustbox}
  \caption{Class-level training and prediction accuracy on unknown data within a system.}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_class_within_graph}
\end{figure}

The class-level training and prediction accuracy of unknown data within a system is shown in Figure~\ref{fig:prediction_class_within_graph}. We can see that a number of the test subjects have large variations in their accuracy, and in four cases actually hit 0\%. These 0\% situation indicate that nothing was correctly predicted, we how this situation might be occurring in Section~\ref{subsec:experiment_optimization_generalization}. We also can note that half of the test subjects for class-level prediction of unknown data within a system had a performance worse than random. The average prediction accuracy of for this experiment is 31.7\%\pm10.2\%, which is lower than random with a large standard deviation.

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_method_within_graph}
  \end{adjustbox}
  \caption{Method-level training and prediction accuracy on unknown data within a system.}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_method_within_graph}
\end{figure}

The method-level training and prediction accuracy of unknown data within a system is shown in Figure~\ref{fig:prediction_method_within_graph}. The mean average prediction accuracies of the test subject vary, which suggests that the prediction quality might depending on the project itself, or that more data is required for this experiment. While \textit{joda-primatives}'s mean prediction accuracy for class-level was 0\%, the mean accuracy for method-level was approximately 90\%. It seems to be that \textit{joda-primatives} represents an outlier in our test subjects. All mean prediction accuracies exceed random for method-level predictions, while class-level predictions did not fare as well. This suggests that class-level predictions are harder to predict because:
\begin{itemize}
  \item Classes have much more factors involved in them (i.e., a set of methods) thus harder to predict.
  \item Our approach does not account for overloaded, anonymous, and abstract methods thus the classes are partially incomplete in data.
  \item For our experiment we had considerably less class-level data available than that of method-level data.
\end{itemize}
The average prediction accuracy of for this experiment is 53.2\%\pm5.1\%, which is higher than random with half the standard deviation of the class-level predictions.


\subsubsection{Prediction Across Systems}
\label{subsubsec:prediction_across_systems}
This experiment explores the capability of predicting unknown source code units across a software system. Whether predictions will fare as well as predictions within a system (see Section~\ref{subsubsec:prediction_within_system} is explored in this section.

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_class_across_graph}
  \end{adjustbox}
  \caption{Class-level training and prediction accuracy on unknown data across systems.}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_class_across_graph}
\end{figure}


The class-level training and prediction accuracy of unknown data across a system is shown in Figure~\ref{fig:prediction_class_across_graph}. Of the eight test subjects three of them are below random with respect to mean prediction accuracy. With respect to Figure~\ref{fig:prediction_class_within_graph}, the class-level prediction accuracy within a system has much more variation in accuracy than across systems. The less variation in prediction accuracy across systems is most likely due to the fact of having more data items present as we now consider seven of the test subjects for training data. The average prediction accuracy of for this experiment is 34.4\%\pm4.7\%, which is slightly higher than random and is an improvement over the class-level within systems (only because the within systems had a 0\% for \textit{joda-primatives}, otherwise within a system average accuracy would have been 36.2\%).

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_method_across_graph}
  \end{adjustbox}
  \caption{Method-level training and prediction accuracy on unknown data across systems.}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_method_across_graph}
\end{figure}

The method-level training and prediction accuracy of unknown data across a system is shown in Figure~\ref{fig:prediction_method_across_graph}. Of the test subjects all but \textit{joda-primatives} have a mean accuracy that exceeds random. With respect to Figure~\ref{fig:prediction_method_within_graph}, the method-level prediction accuracy within a system has much more variation in accuracy than across systems. The less variation in prediction accuracy across systems is most likely due to the fact of having more data items present as we now consider seven of the test subjects for training data. The average prediction accuracy of for this experiment is 37.6\%\pm2.6\%, which is slightly higher than random. With respect to the prediction accuracy within a system for method-level source code units this experiment has shown a drop of 15.6\% in prediction accuracy. This drop in accuracy suggests that prediction across systems is a more challenging prediction to make regarding mutation score prediction.


\subsection{Optimization and Generalization}
\label{subsec:experiment_optimization_generalization}
\begin{quote}
  \emph{\textbf{Research Question \#1:} Can we optimize our approach to achieve better performance by using a different measure of classifier performance?}
\end{quote}

\begin{quote}
  \emph{\textbf{Research Question \#2:} Can we identify a general set of \gls{svm} parameters that maximize mutation score prediction performance on unknown data?}
\end{quote}

\noindent
We kept track of the frequency of parameter pairs selected over all the prediction experiments conducted in Section~\ref{subsec:experiment_prediction}. As a result of \emph{LIBSVM}'s \emph{easy script}, we saw 57 different pairings of the \emph{LIBSVM} parameters \emph{cost} and \emph{gamma} (described in Section~\ref{subsec:background_support_vector_machine}). This indicates that the classifiers are being tuned specifically to maximize the cross-validation accuracy. Due to undersampling, different parameters are being used to ensure this maximization of cross-validation accuracy. To encourage generalization of unknown projects and data, ideally we want to find a parameter pairing that maximizes generalizability and classification performance on unknown data. As our approach initially requires mutation testing results to perform training it might be appropriate to select the best parameters for the given data. In the previous section for predictions on unknown data (see Section~\ref{subsec:experiment_prediction}), we explored predictions within and across systems. For this experiment we continue with the same setup by considering across and within systems. Regarding this experiment it becomes much more clear on why we need a generalizable set of parameters that hopefully performance well on most test subjects. In terms of usability if we can find a general set of parameters for predicting mutation scores, this will lessen the need to specifically tune every classifier prior to prediction.

\begin{sidewaysfigure}[!tb]
  \centering
  \caption{Raw output of training on \emph{joda-time} then predicting on its unknowns using the parameters \emph{cost}=0.03125 and \emph{gamma}=0.0078125.}
  \label{fig:raw_output_bad}
  \begin{minipage}{.95\textheight}
  \scriptsize{
  \lstinputlisting[language=Java, basicstyle=\fontsize{3.3mm}{4.3mm}\ttfamily,]{listings/raw_output_bad.txt}
  }
  \end{minipage}
  
  \vspace{3em}

  \centering
  \caption{Raw output of training on \emph{joda-time} then predicting on its unknowns using the parameters \emph{cost}=8 and \emph{gamma}=0.125.}
  \label{fig:raw_output_good}
  \begin{minipage}{.95\textheight}
  \scriptsize{
  \lstinputlisting[language=Java, basicstyle=\fontsize{3.3mm}{4.3mm}\ttfamily,]{listings/raw_output_good.txt}
  }
  \end{minipage}
\end{sidewaysfigure}
\afterpage\clearpage

We noticed that in some situations accuracy is not the best measure for a classifier's effectiveness. For example, consider the following situation: Given imbalanced data for the testing/unknown data set, the accuracy could misrepresent the performance of the classifier. The raw outputs of our classifier using two different sets of parameters on the \emph{joda-time} subject are shown in Figures~\ref{fig:raw_output_bad}~and~\ref{fig:raw_output_good}. In both of the raw outputs we can see a confusion matrix along with performance measures. We can see in the raw output in Figure~\ref{fig:raw_output_bad} that all of predictions fall in category $2$. The \emph{joda-time} data set is imbalanced with the majority of data (i.e., 76.2\% of the data) belonging to category $2$. Even with the biased predictions made towards the majority category, the accuracy of the prediction is 76.2\%. In contrast to the the raw output presented in Figure~\ref{fig:raw_output_good} we can see that the accuracy is slightly lower at 71.7\%. Now even though the accuracy in the second example is slightly lower we can consider it a superior classifier to the former as it actually treats the categories in a more unbiased fashion (i.e., one category is not receiving the majority of predictions like the previous example). To alleviate this problem, we consider other measurements that can be used to assess the predictive capabilities of classifiers, specifically the following measures:

\begin{itemize}
  \item \textbf{F-score} represents the harmonic mean of the recall and precision for a category~\cite{SJS06}. A score closer to 1 (i.e., 100\%) represents better performance.
  \begin{equation}
    \emph{$\text{F-score} = 2*\frac{recall * precision}{recall + precision}$}
    \label{equ:f_score}
  \end{equation}

  \item \textbf{Balanced Accuracy} represents the average accuracy obtained on the category~\cite{BOSB10, SJS06}. A score closer to 1 (i.e., 100\%) represents better performance.
  \begin{equation}
    \emph{$\text{balanced accuracy} = \frac{recall + specificity}{2}$}
    \label{equ:balanced_area_under_curve}
  \end{equation}

  \item \textbf{Youden's Index} represents the classifier's ability to avoid failure~\cite{SJS06}. It can also be calculated using the balanced accuracy. A score closer to 1 (i.e., 100\%) represents better performance.
  \begin{equation}
    \emph{$\text{youden's index} = recall - ( 1 - specificity)$}
    \label{equ:youden_index_a}
  \end{equation}
  \begin{equation}
    \emph{$\text{youden's index} = 2 * \text{balanced accuracy} - 1$}
    \label{equ:youden_index_b}
  \end{equation}
\end{itemize}

\begin{table}[!tb]
  \centering
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{3.5cm}|>{\raggedleft\arraybackslash}p{3.5cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Performance Measure} & \textbf{Bad Classifier (Figure~\ref{fig:raw_output_bad})} & \textbf{Good Classifier (Figure~\ref{fig:raw_output_good})} \\
      \hline Mean Accuracy & 76.2\% & 71.7\% \\
      \hline Mean F-score & 28.8\% & 45.4\% \\
      \hline Mean Balance Accuracy & 50.0\% & 62.3\% \\
      \hline Mean Youden-index & 00.0\% & 24.6\% \\
      \hline
    \end{tabular}
  \end{threeparttable}
  \caption{Comparison of performance measures for a \emph{bad} classifier vs. a \emph{good} classifier.}
  \vspace{2mm}
  \hrule
  \label{tab:experiments_comparison_measures}
\end{table}

Using their accuracy, the \emph{bad} and \emph{good} classifiers were unable to distinguish the better classifier (i.e., fair predictions over all categories) while the three aforementioned performance measures are capable of doing so. We take the average value of the performance measures (i.e., the sum divided by the three categories for each measure) and compare classifiers in Table~\ref{tab:experiments_comparison_measures}. The comparison shows that the new performance measures better reflect the performance of the classifier than traditional accuracy in all three cases.

To further generalize our predictions, we conducted our own grid search with comparisons to prediction accuracy  instead of cross-validation accuracy. As briefly mentioned in Section~\ref{subsec:experiment_tool_configuration} a grid search performance search over the parameters, which in our case is \emph{cost} and \emph{gamma} the two \gls{svm} parameters. We use a coarse search over the parameter ranges between 0.00001 and 10000 by adjusting the order of magnitude by a factor of ten. The following outlines our strategy to find the pairing of parameters that maximizes the performance of our \gls{svm} on predicting unknown data:

\begin{enumerate}
  \item Grid search using coarse parameter ranges between 0.00001 and 10000 by adjusting the order of magnitude by a factor of ten.
  \item We maximize the F-score (we could have used Balance Accuracy or Youden-index) on unknown data (i.e., what remains after undersampling or the excluded test subject) instead of on cross-validation of the training data.
  \item We conduct the previous two steps (i.e., grid search of an data set) on each of the individual test subjects and also for the \emph{all\_but\_<subject>} subjects. For each test subject we perform ten executions for each parameter pairing to account for  undersampling.
  \item Use a simple rank summation (i.e., ascending rank $n$ has a value of $n$) to tally the parameter pairings that consistently performed the best on the data sets.
  \item We pick the parameter pairing that perform best on both the individual subjects and the \emph{all\_but\_<subject>} subjects.
\end{enumerate}

Using our search strategy as described we attained the following \gls{svm} parameters for the class-level [\emph{cost}=100, \emph{gamma}=0.01] and method-level [\emph{cost}=100, \emph{gamma}=1]. These parameters were found to offered the greatest generalizable over the different data sets. Furthermore by maximizing F-score instead of accuracy these parameters will avoid issues presented by using accuracy alone. Section~\ref{subsubsec:prediction_with_parameters_within_system} presents results for prediction within a system, and Section~\ref{subsubsec:prediction_with_parameters_across_systems} presents results for prediction across systems. Both which utilize the found set of parameters that offer the greatest performance with respect to maximizing F-score on predicting unknown data. A comparison between pre/post generalized parameter prediction performance is explored in Section~\ref{sububsec:comparison_prediction_performance_generalized}.


\subsubsection{Prediction Within a System}
\label{subsubsec:prediction_with_parameters_within_system}
This experiment is the same as the previous one (see Section~\ref{subsubsec:prediction_within_system}) in that we are concerned with assessing the prediction capability within a system. The only difference is that these results are based on a generalized set of parameters found through a grid search (found in Section~\ref{subsec:experiment_optimization_generalization}).

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_with_parameters_class_within_graph}
  \end{adjustbox}
  \caption{Class-level training and prediction accuracy on unknown data within a system using generalized parameters [\emph{cost}=100, \emph{gamma}=0.01].}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_with_parameters_class_within_graph}
\end{figure}

The class-level training and prediction accuracy of unknown data within a system using generalized parameters is shown in Figure~\ref{fig:prediction_with_parameters_class_within_graph}. We can see that a number of the test subjects still have large variations in their accuracy when compared to the same experiment without the generalized parameters (see Figure~\ref{fig:prediction_class_within_graph}). There are no more cases of 0\% mean accuracy anymore which is a good sign that the generalized parameters using F-score actually alleviated this situation. Three of the eight subjects have a mean accuracy lower than random. The average prediction accuracy of for this experiment is 36.7\%\pm10.1\%, which is slightly higher than random and is an improvement of +5.0\% over the non-generalized parameter experiment (see Table~\ref{tab:experiments_comparison_class_within_prediction}).

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_with_parameters_method_within_graph}
  \end{adjustbox}
  \caption{Method-level training and prediction accuracy on unknown data within a system using generalized parameters [\emph{cost}=100, \emph{gamma}=1].}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_with_parameters_method_within_graph}
\end{figure}

The method-level training and prediction accuracy of unknown data within a system using generalized parameters is shown in Figure~\ref{fig:prediction_with_parameters_method_within_graph}. We can see that the results are similar when compared to the same experiment without the generalized parameters (see Figure~\ref{fig:prediction_method_within_graph}). The average prediction accuracy of for this experiment is 56.8\%\pm6.2\%, which is higher than random and is an improvement of +3.6\% over the non-generalized parameter experiment (see Table~\ref{tab:experiments_comparison_method_within_prediction}).

\subsubsection{Prediction Across Systems}
\label{subsubsec:prediction_with_parameters_across_systems}
This experiment is the same as the previous one (see Section~\ref{subsubsec:prediction_across_system}) in that we are concerned with assessing the prediction capability across systems. The only difference is that these results are based on a generalized set of parameters found through a grid search (found in Section~\ref{subsec:experiment_optimization_generalization}).

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_with_parameters_class_across_graph}
  \end{adjustbox}
  \caption{Class-level training and prediction accuracy on unknown data across systems using generalized parameters [\emph{cost}=100, \emph{gamma}=0.01].}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_with_parameters_class_across_graph}
\end{figure}

The class-level training and prediction accuracy of unknown data across systems using generalized parameters is shown in Figure~\ref{fig:prediction_with_parameters_class_across_graph}. When compared to the same experiment without the generalized parameters (see Figure~\ref{fig:prediction_class_across_graph}) we can see slight improvements in terms of mean accuracy, in particular there are only 2 test subjects with less than random mean accuracy. The average prediction accuracy of for this experiment is 39.0\%\pm3.9\%, which is slightly higher than random and is an improvement of +4.6\% over the non-generalized parameter experiment (see Table~\ref{tab:experiments_comparison_class_across_prediction}).

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={\textwidth}{\textheight}}
    \input{plots/prediction_with_parameters_method_across_graph}
  \end{adjustbox}
  \caption{Method-level training and prediction accuracy on unknown data across systems using generalized parameters [\emph{cost}=100, \emph{gamma}=1].}
  \vspace{2mm}
  \hrule
  \label{fig:prediction_with_parameters_method_across_graph}
\end{figure}

The method-level training and prediction accuracy of unknown data across systems using generalized parameters is shown in Figure~\ref{fig:prediction_with_parameters_method_across_graph}. We can see that the results are similar when compared to the same experiment without the generalized parameters (see Figure~\ref{fig:prediction_method_within_graph}). The average prediction accuracy of for this experiment is 42.8\%\pm1.8\%, which is higher than random and is an improvement of +5.2\% over the non-generalized parameter experiment (see Table~\ref{tab:experiments_comparison_method_across_prediction}).

\begin{sidewaystable}[!tb]
  \centering
  \caption{Comparison of class-level prediction accuracy within systems (mean $\pm$ standard deviation) before/after generalized parameters are used.}
  \label{tab:experiments_comparison_class_within_prediction}
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Before Parameter Generalization (\%) (Figure~\ref{fig:prediction_class_within_graph})} & \textbf{After Parameter Generalization (\%) (Figure~\ref{fig:prediction_with_parameters_class_within_graph})} & \textbf{Gain($\uparrow$)/Lost($\downarrow$) from Parameter Generalization (\%)} \\
      \hline \emph{logback-core} & 54.3\pm3.9 & 39.9\pm12.5 & $\downarrow$14.4\pm$\uparrow$8.6 \\
      \hline \emph{barbecue} & 34.4\pm15.5 & 40.0\pm9.2 & $\uparrow$5.6\pm$\downarrow$6.3 \\
      \hline \emph{jgap} & 38.1\pm20.5 & 47.0\pm7.5 & $\uparrow$8.9\pm$\downarrow$13.0 \\
      \hline \emph{commons-lang} & 27.0\pm17.9 & 30.3\pm11.3 & $\uparrow$3.3\pm$\downarrow$6.6 \\
      \hline \emph{joda-time} & 42.7\pm6.2 & 41.6\pm6.6 & $\downarrow$1.1\pm$\uparrow$0.4 \\
      \hline \emph{openfast} & 28.1\pm5.1 & 32.1\pm5.3 & $\uparrow$4.0\pm$\uparrow$0.2 \\
      \hline \emph{jsoup} & 28.6\pm12.6 & 33.9\pm10.4 & $\uparrow$5.3\pm$\downarrow$2.2 \\
      \hline \emph{joda-primitives} & 0.0\pm0.0 & 28.9\pm17.7 & $\uparrow$28.9\pm$\uparrow$17.7 \\
      \hline \textbf{average} & \textbf{31.7\pm10.2} & \textbf{36.7\pm10.1} & $\uparrow$\textbf{5.0\pm}$\downarrow$\textbf{0.1} \\
      \hline
    \end{tabular}
  \end{threeparttable}

  \vspace{3em}

  \centering
  \caption{Comparison of class-level prediction accuracy across systems (mean $\pm$ standard deviation) before/after generalized parameters are used.}
  \label{tab:experiments_comparison_class_across_prediction}
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Before Parameter Generalization (\%) (Figure~\ref{fig:prediction_class_across_graph})} & \textbf{After Parameter Generalization (\%) (Figure~\ref{fig:prediction_with_parameters_class_across_graph})} & \textbf{Gain($\uparrow$)/Lost($\downarrow$) from Parameter Generalization (\%)} \\
      \hline \emph{all\_but\_logback-core} & 29.0\pm3.7 & 38.1\pm2.7 & $\uparrow$9.1\pm$\downarrow$1.0 \\
      \hline \emph{all\_but\_barbecue} & 36.1\pm6.4 & 31.6\pm4.8 & $\downarrow$4.5\pm$\downarrow$1.6 \\
      \hline \emph{all\_but\_jgap} & 34.0\pm6.1 & 41.5\pm2.0 & $\uparrow$7.5\pm$\downarrow$4.1 \\
      \hline \emph{all\_but\_commons-lang} & 32.1\pm2.1 & 32.9\pm3.0 & $\uparrow$0.8\pm$\uparrow$0.9 \\
      \hline \emph{all\_but\_joda-time} & 35.6\pm4.9 & 48.6\pm2.3 & $\uparrow$13.0\pm$\downarrow$2.6 \\
      \hline \emph{all\_but\_openfast} & 37.4\pm2.6 & 39.7\pm3.2 & $\uparrow$2.3\pm$\uparrow$0.6 \\
      \hline \emph{all\_but\_jsoup} & 44.7\pm5.8 & 43.0\pm5.0 & $\downarrow$1.7\pm$\downarrow$0.8 \\
      \hline \emph{all\_but\_joda-primitives} & 26.3\pm6.1 & 36.7\pm8.1 & $\uparrow$10.4\pm$\uparrow$2.0 \\
      \hline \textbf{average} & \textbf{34.4\pm4.7} & \textbf{39.0\pm3.9} & $\uparrow$\textbf{4.6\pm}$\downarrow$\textbf{0.8} \\
      \hline
    \end{tabular}
  \end{threeparttable}
\end{sidewaystable}
\afterpage\clearpage

\begin{sidewaystable}[!tb]
  \centering
  \caption{Comparison of method-level prediction accuracy within systems (mean $\pm$ standard deviation) before/after generalized parameters are used.}
  \label{tab:experiments_comparison_method_within_prediction}
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Before Parameter Generalization (\%) (Figure~\ref{fig:prediction_method_within_graph})} & \textbf{After Parameter Generalization (\%) (Figure~\ref{fig:prediction_with_parameters_method_within_graph})} & \textbf{Gain($\uparrow$)/Lost($\downarrow$) from Parameter Generalization (\%)} \\
      \hline \emph{logback-core} & 42.4\pm5.7 & 47.6\pm10.9 & $\uparrow$5.2\pm$\uparrow$5.2 \\
      \hline \emph{barbecue} & 45.4\pm8.0 & 52.3\pm8.7 & $\uparrow$6.9\pm$\uparrow$0.7 \\
      \hline \emph{jgap} & 43.4\pm5.1 & 53.4\pm7.2 & $\uparrow$10.0\pm$\uparrow$2.1 \\
      \hline \emph{commons-lang} & 55.8\pm3.7 & 52.2\pm2.3 & $\downarrow$3.6\pm$\downarrow$1.4 \\
      \hline \emph{joda-time} & 63.0\pm3.4 & 67.5\pm5.6 & $\uparrow$4.5\pm$\uparrow$2.2 \\
      \hline \emph{openfast} & 48.3\pm4.3 & 51.0\pm5.6 & $\uparrow$2.7\pm$\uparrow$1.3 \\
      \hline \emph{jsoup} & 36.8\pm8.0 & 43.2\pm7.6 & $\uparrow$6.4\pm$\downarrow$0.4 \\
      \hline \emph{joda-primitives} & 90.1\pm2.7 & 87.1\pm1.5 & $\downarrow$3.0\pm$\downarrow$1.2 \\
      \hline \textbf{average} & \textbf{53.2\pm5.1} & \textbf{56.8\pm6.2} & $\uparrow$\textbf{3.6\pm}$\uparrow$\textbf{1.1} \\
      \hline
    \end{tabular}
  \end{threeparttable}

  \vspace{3em}

  \centering
  \caption{Comparison of method-level prediction accuracy across systems (mean $\pm$ standard deviation) before/after generalized parameters are used.}
  \label{tab:experiments_comparison_method_across_prediction}
  \rowcolors{1}{gray!30}{gray!20}
  \begin{threeparttable}
    \begin{tabular}{|l|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|>{\raggedleft\arraybackslash}p{4cm}|}
      \rowcolor[RGB]{169,196,223}
      \hline \textbf{Test Subject} & \textbf{Before Parameter Generalization (\%) (Figure~\ref{fig:prediction_method_across_graph})} & \textbf{After Parameter Generalization (\%) (Figure~\ref{fig:prediction_with_parameters_method_across_graph})} & \textbf{Gain($\uparrow$)/Lost($\downarrow$) from Parameter Generalization (\%)} \\
      \hline \emph{all\_but\_logback-core} & 34.3\pm1.9 & 37.6\pm1.7 & $\uparrow$3.3\pm$\downarrow$0.2 \\
      \hline \emph{all\_but\_barbecue} & 41.7\pm4.6 & 46.9\pm2.6 & $\uparrow$5.2\pm$\downarrow$2.0 \\
      \hline \emph{all\_but\_jgap} & 41.6\pm2.0 & 46.9\pm1.3 & $\uparrow$5.3\pm$\downarrow$0.7 \\
      \hline \emph{all\_but\_commons-lang} & 37.6\pm2.1 & 46.6\pm2.5 & $\uparrow$9.0\pm$\uparrow$0.4 \\
      \hline \emph{all\_but\_joda-time} & 35.4\pm2.1 & 43.7\pm1.6 & $\uparrow$8.3\pm$\downarrow$0.5 \\
      \hline \emph{all\_but\_openfast} & 35.9\pm2.2 & 44.3\pm1.6 & $\uparrow$8.4\pm$\downarrow$0.6 \\
      \hline \emph{all\_but\_jsoup} & 42.5\pm1.3 & 47.7\pm1.0 & $\uparrow$5.2\pm$\downarrow$0.3 \\
      \hline \emph{all\_but\_joda-primitives} & 31.9\pm4.2 & 28.8\pm2.0 & $\downarrow$3.1\pm$\downarrow$2.2 \\
      \hline \textbf{average} & \textbf{37.6\pm2.6} & \textbf{42.8\pm1.8} & $\uparrow$\textbf{5.2\pm}$\uparrow$\textbf{0.8} \\
      \hline
    \end{tabular}
  \end{threeparttable}
\end{sidewaystable}
\afterpage\clearpage

\subsubsection{The Effects of Generalized Parameters on Prediction Performance} 
\label{sububsec:comparison_prediction_performance_generalized}
Using the generalizable parameters we can see that in both class- and method-level results, the resulting accuracy tend to increase slightly. In some situations we can even see that a decreased variability in accuracy. In particular we can see that in class-level predictions, the possibilities of 0\% accuracy (which occurred in four of the test subjects without the generalizable parameters) no longer occurs. This happened as a result of the selecting parameters that maximized F-score instead, which treats the predictions of categories more fairly (i.e., avoiding predicting all of one category, which could be the wrong category). To further see the benefits of using generalized \emph{LIBSVM} parameters we compared the individual accuracies and standard deviation of each test subject.

As presented in Tables~\ref{tab:experiments_comparison_class_within_prediction}~--~\ref{tab:experiments_comparison_method_across_prediction}, we can see the gains and losses in mean and standard deviation of prediction accuracy from the application of generalized parameters. In terms of changes, improvement for mean accuracy would be a gain (i.e., better prediction accuracy) while for standard deviation an improvement would be a loss (i.e., smaller variation in prediction accuracy). Of the 16 class-level test subjects presented in Table~\ref{tab:experiments_comparison_class_within_prediction}~and~\ref{tab:experiments_comparison_class_across_prediction}, 12 out of 16 test subjects saw an improvement in mean accuracy and 9 out of 16 test subjects saw an improvement in standard deviation. Of the 16 method-level test subjects presented in Table~\ref{tab:experiments_comparison_method_within_prediction}~and~\ref{tab:experiments_comparison_method_across_prediction}, 13 out of 16 test subjects saw an improvement in mean accuracy and 10 out 16 test subjects saw an improvement in standard deviation. These results show that the generalized parameters overall had a positive effect on the test subjects's performance. 

Overall, the following summarizes the results of using generalized parameters for our approach on prediction of unknown data within and across systems:
\begin{itemize}
  \item Class-level average prediction accuracy on unknown data within systems saw an improvement of +5.0\% with an improvement of -0.1\% in standard deviation. Resulting in a average prediction accuracy of 36.7\%\pm10.1\%.
  \item Class-level average prediction accuracy on unknown data across systems saw an improvement of +4.6\% with an improvement of -0.8\% in standard deviation. Resulting in a average prediction accuracy of 39.0\%\pm3.9\%.
  \item Method-level average prediction accuracy on unknown data within systems saw an improvement of +3.6\% with a decline in standard deviation of +1.1. Resulting in a average prediction accuracy of 56.8\%\pm6.2\%.
  \item Method-level average prediction accuracy on unknown data within systems saw an improvement of +5.2\% with a decline in standard deviation of +0.8. Resulting in a average prediction accuracy of 42.8\%\pm1.8\%.
\end{itemize}
With respect to predictions in general, method-level prediction have a higher mean accuracy in all situations (i.e., within and across systems). Furthermore, predictions within systems tend to high higher standard deviation than predictions across systems. This difference in standard deviation most likely is attributed to the abundance of data items present for training and prediction. With respect to prediction accuracy the class-level predictions actually performed better across systems only by a difference of 2.3\%, while method-level predictions performed better within systems by a difference of 14\%.

The improvements in both class- and method-level are both a side benefit of using generalized \emph{LIBSVM} parameters, as the main purpose was to nullify the need for parameter selection (i.e., no need to grid search on known data) to make predictions on unknown data. After optimizations and generalization, our approach for mutation score prediction using source code and test suite metrics can out perform random in nearly all test subjects observed. 

\subsection{Impact of Training Data Availability on Prediction Accuracy}
\label{subsec:experiment_data}
\begin{quote}
  \emph{\textbf{Research Question \#1:} How is the prediction accuracy impacted by the availability of training data?}
\end{quote}

\begin{quote}
  \emph{\textbf{Research Question \#2:} Is it possible to only train on a fraction of the source code units and achieve approximately the same prediction performance on the remaining source code units?}
\end{quote}

\noindent
As we saw in the previous section using our approach we achieve 56.8\% prediction accuracy of method-level source code units within systems. This result exceeds random by 23.5\% and therefore shows that our approach is capable of predicting mutation score categories using source code and test suite metrics at least within systems. As mentioned in the thesis statement, \emph{``The predictions can be used to reduce the resource cost of mutation testing in traditional iterative development.''}. Iterative development is a software development life cycle that allows developers to work on small changes which eventually adds to a large change with respect to a software system. Traditional iterative development may involve expanding/reducing/refactoring the \gls{sut}, and/or attempting to improve the test suite. By including mutation testing between iterations to determine if any improvements have occurred can be costly if done in a naive manner (i.e., re-conduct the whole mutation test process using the new version of the \gls{sut}). Even with an intelligent approach of selective mutation (i.e., only mutation testing source code units that were added/removed/modified since the previous iteration), the cost of mutation testing can still be substantial.

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={.95\textwidth}{.90\textheight}}
    \input{plots/divisor_class_graph}
  \end{adjustbox}
  \caption{Class-level prediction accuracies of each test subject using training and prediction with various amounts of training data.}
  \vspace{2mm}
  \hrule
  \label{fig:divisor_class_graph}
\end{figure}

\begin{figure}[!tb]
  \centering
  \begin{adjustbox}{max size={.95\textwidth}{.90\textheight}}
    \input{plots/divisor_method_graph}
  \end{adjustbox}
  \caption{Method-level prediction accuracies of each test subject using training and prediction with various amounts of training data.}
  \vspace{2mm}
  \hrule
  \label{fig:divisor_method_graph}
\end{figure}

There is no consensus on good ratios for training data and testing data. Using the common $10$-fold cross-validation~\cite{Koh95} as a guideline, a 9:1 ratio for training data to testing data is a good rule-of-thumb. Using 90\% of a large data set for training might be considered wasteful when considering a classifier's \emph{learning curve}. Provost et al. mention \emph{``Learning curves typically have a steeply sloping portion early in the curve, a more gently sloping middle portion, and a plateau late in the curve''}~\cite{PJO99}. Using a sample of the available training data that provides a representative of the whole data set will retain near the same predictive performance. Through an empirical evaluation, Provost et al. demonstrated that the minimum amount of training data required to maximize the prediction performance of a classifier (i.e., reaching the plateau of the learning curve) varies based on the data set. Using a progress random sampling algorithm, Provost et.al. were able to determine the minimum amount of training data for their data sets, they concluded that the minimum amount of data required is different for each data set. From their experimentation with three data sets they found that the minimum percent of training data required for maximum prediction performance was 2\% (of 100000 items), 12\% (of 100000 items) and 25\% (of 32000 items). Finally Provost et al. were able to demonstrate the benefit of random sampling to reduce the training set, namely a reduction in computational cost using a smaller training data set. Wang et al. also demonstrated random sampling reduce the amount of training data necessary to achieve maximum prediction performance~\cite{WNC05}. Wang et al. evaluated a range of random sampling (ranging from 10\%-100\%) of four data sets and the greatest loss in prediction performance was approximately 10.5\%. 

Random sampling appears to perform well for reducing the training set size while retaining prediction performance~\cite{PJO99, WNC05}. There exist more complex approaches to this problem, one uses centroids of weighted clusters which essentially groups similar items in the training set and treating them as one item~\cite{NBP08}. With our approach with respect to data availability, we are interested in minimizing the data required to make accuracy predictions. Similar to the previous research on random sampling to reduce the training data set, we decided to explore how it affects our prediction accuracy. Ideally we can reduce the resource cost of mutation testing in traditional iterative development with intermixed iterations of mutation testing and predictions by performing mutation testing on a portion of the \gls{sut} and predicting the reminding portion. 

We conducted a number of training and prediction executions using different amounts of source code units for training. We took the amount of undersampled training data points and divided this amount by intervals of 0.5 from 1.0 to 10.0. We conducted ten executions using the generalized parameters from Section~\ref{subsec:experiment_optimization_generalization} for each new training amount and recorded the mean accuracy. In situations where the new training amount was identical to another's interval their resulting accuracy were averaged. As we can see in Figure~\ref{fig:divisor_class_graph}, the class-level source code units did not show much information as there was a limited number of unique points for the test subjects. This is due to the limited data set available for the test subjects, recalling \emph{barbecue} only had two data points per category, while \emph{joda-primitives} only had one. Unfortunately there does not seem to be enough data to warrant any observation from the class-level. With Figure~\ref{fig:divisor_method_graph} we can see an apparent trend for method-level source code units, and there appears to be a $log(n)$ relationship with prediction accuracy and the amount of data used for training (i.e., the \emph{learning curve}). \emph{joda-primitives} shows exactly the trend were expecting: the prediction accuracy tapers off reaching its maximum value between 30\%--35\%. Looking at other test subjects we can see similar behaviour, though not as pronounced.

We know that there exists a minimum number of training data points required to reach the prediction accuracy plateau, however with our data sets we might not have enough data to see this effect. Considering that the previous research works see results with a little as 2\% of the training data used, we cannot possibly achieve results like that considering we have less than 1000 items for our individual test subjects comparative to 100000 items. A quick observation of our results suggests that we could probably use a fraction of the available data from a \gls{sut} to achieve near optimal prediction accuracy. In our case we would suggest using one third or more of the available data for training purposes to maximize prediction accuracy. By considering a fraction of the mutants for training purposes it is not necessary to evaluate the remaining fraction and our approach could be used to predict the mutation score of the remaining source code units. To account for the potential mis-classifications (considering we have approximately 50\% prediction accuracy) it would be best to cycle the training data such that the we select a new random sample that is mostly unique in each iteration. With enough iterations all mutants would have been actually evaluated once, while only really evaluating a fraction of the mutants from the \gls{sut}. As a side effect we can assume that at some point all mutants have actually been evaluated and we could keep this information in our database and reuse it for training purposes.


\section{Threats to Validity}
\label{sec:experiment_threats}
We consider the four categories for threats to validity with respect to experimentation in Section~\ref{subsec:experiment_conclusion_validity}~to~\ref{subsec:experiment_external_validity}.


\subsection{Conclusion Validity}
\label{subsec:experiment_conclusion_validity}
Threats to conclusion validity involve issues with the process and statistical means to draw any conclusions regarding experiments~\cite{WRH+00,WKP10}. We utilized various summary statistical measures to determine the conclusions of our results. In particular, we used mean, standard deviation, quartiles and frequencies to understand our experiments with respect to their results. Furthermore, with our results we conducted a minimum of ten executions per experiment to mitigate the randomness of our results. With respect to drawing conclusions, we compared the average accuracy to what a random prediction would achieve. Thus by comparing the mean accuracies we were able to compare our approach to random. In retrospect, we should have performed more executions per experiment to further reduce the noise. Furthermore we could have performed a statistical test to understand the statistical significants of our comparison.


\subsection{Internal Validity}
\label{subsec:experiment_internal_validity}
Internal threats to validity are concerned with factors that could influence the independent variable in our experiments~\cite{WRH+00,WKP10}. Our independent variables are the features themselves from the eight open source software systems that we selected. Obviously there could be issues that can arise based on the measures that our tools returned for each software system, though these tools are well established and provide simplistic measures (i.e., issues are unlikely to arise due to \emph{incorrect} results). With respect to the mutants themselves that are generated by the \emph{Javalanche} mutation testing tool, the version used was experimental and could be more susceptible to \emph{incorrect} results. Furthermore \emph{Javalanche} uses a subset of mutants (i.e., mainly method-level mutants), which could have a major impact on the class-level source code units. With respect to \emph{true} internal validity the independent variables are not influencing each other in ways that we were not aware of that could be detrimental to our experiment.


\subsection{Construct Validity}
\label{subsec:experiment_construct_validity}
Whether the independent and dependant variables we are using actually align with the problem with which we are experimenting is an issue with construct validity~\cite{WRH+00,WKP10}. In our experiment we are using a set of features extracted from open source software systems (i.e., the independent variables) to determine the accuracy of predicting mutation score (i.e, the dependant variable). Machine learning performance measures (i.e., accuracy, F-score, etc\ldots) are valid dependant variables as they measure the effectiveness of the classification technique. The independent variables for machine learning are harder to determine by nature, there is often no clear set of features for making predictions. For our experiment, we observed the two main components involved in mutation testing, the source code and test suite. These two components can be represented in quantifiable metrics (i.e., source code and test suite metrics) which are commonly known and used in Software Engineering research.


\subsection{External Validity}
\label{subsec:experiment_external_validity}
With experiments, one of the major concerns involves the ability for the results to generalize outside of the study. That is external validity~\cite{WRH+00,WKP10}. With our experiment we specifically avoided toy-problems and opted to use open source software systems, which are real software systems. These software systems are not industrial nor are they extremely large-scale (i.e., 100000+ \gls{sloc}), thus we are unsure if the results would generalize to such software systems. The test subjects we chose had some variation in domain (i.e., library, framework, etc\ldots) though our set obviously does not act as a representative of different domains. In addition, most of the test subjects we used had relatively \emph{good} test suites (i.e., of the covered mutants the mutation scores were above 73\% except for one test subject). Due to this, we are unsure how our prediction would perform on software systems with \emph{poor} test suites. Furthermore we observed only the Java language, whether these results generalize to other languages has not been verified. As stated by Kitchenham and Mendes \emph{``It is invalid to select one or two data sets to `prove' the validity of a new technique because we cannot be sure that, of the many published data sets, those chosen are the only ones that favour the new technique''}~\cite{KM09}. We used only eight open source projects as our data sets for our prediction technique, however even though this is more then one or two it is still quite limited.  Mutants can be influenced by external factors such test suite size and mutation operators as it was found that class-level mutants are harder to detect than traditional method-level mutants~\cite{NK11}. As we used only traditional mutation operators this could have an impact on the generalizability, along with the varying sizes of the test suites of our test subjects.

Recall that our approach for predicting mutation scores based on source code and test suite metrics utilizes a number of tools:

\begin{itemize}
  \item \emph{Javalanche} to collect mutation scores.
  \item \emph{Eclipse Metrics Plugin} to collect source code and test suite metrics.
  \item \emph{EMMA} to collect additional test suite coverage metrics.
  \item \emph{LIBSVM} to perform the training and prediction of the source code units.
\end{itemize}

We selected tools based on the metrics they could provide as well as the the output format, yet there might be other tools that could have performed better. In particular, the mutation testing tool we selected is not the newest, and omits a whole class of mutations (i.e., class-level object-oriented mutants), which could be misrepresenting the mutation scores. The tools used to collect the features of the source code units might not be comprehensive in terms of features that describe the source code units.


\section{Summary}
\label{sec:experiment_summary}
In this chapter we covered the following topics that demonstrate out approach on several test subjects:

\begin{itemize}
  \item In Section~\ref{sec:experiment_setup} we covered our experimental setup with respect to environment, test subjects, tool configuration and data preprocessing.
  \item In Section~\ref{sec:experiment_method} we discussed our experimental method for the five experiments that were conducted in this chapter.
  \item In Section~\ref{sec:experiment_results} we covered a number of experiments and discussed their results. Specifically we experimented with mutation score distribution (Section~\ref{subsec:experiment_mutation_score_distribution}), cross-validation (Section~\ref{subsec:experiment_cross_validation}), prediction (Section~\ref{subsec:experiment_prediction}), optimization and generalization (Section~\ref{subsec:experiment_optimization_generalization}), and the impact of data availability (Section~\ref{subsec:experiment_data}).
  \item In Section~\ref{sec:experiment_threats} we discussed conclusion, internal, construct and external threats to validity of our empirical evaluation.
\end{itemize}
