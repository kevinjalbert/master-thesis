\chapter{Approach}
\label{chap:approach}
This chapter describes our approach to determine the mutation score of a source code unit under test based on source code and test suite metrics. In Section~\ref{sec:approach_tools} we first describe the tools and technologies used in our approach. Section~\ref{sec:approach_process} details the complete process for collecting our initial data and training using the \gls{svm}. Then in Section~\ref{sec:approach_prediction} we describe how we use a SVM for prediction.


\section{Tools}
\label{sec:approach_tools}
The following sections show the tools selected for the technologies we are using in our approach. We selected our tools based on the ability to easily synthesis data to-and-from the tools. In addition, we preferred tools that give access to a \gls{cli}.


\subsection{Javalanche}
\label{subsec:approach_javalanche}
In our research we use Javalanche (version 0.4), a mutation testing tool for Java~\cite{SZ09} that applies a subset of the method-level mutation operators. Javalanche uses a subset of the method-level mutation operators (see Table~\ref{tab:javalanche_operators}). These selected operators provide a close approximation of the effectiveness of using the entire set of method-level operators at a reduced cost~\cite{OLR+96}.

\begin{table}[h]
  \centering
  \rowcolors{2}{gray!30}{gray!20}
  \begin{tabular}{|l|l|}
    \hline
    \rowcolor[RGB]{169,196,223}
    \textbf{Name} & \textbf{Description} \\
    \hline REPLACE\_CONSTANT & Replace a constant \\
    \hline NEGATE\_JUMP & Negate jump condition \\
    \hline ARITHMETIC\_REPLACE & Replace arithmetic operator \\
    \hline REMOVE\_CALL & Remove method call \\
    \hline REPLACE\_VARIABLE & Replace variable reference\\
    \hline ABSOLUTE\_VALUE & Insert absolute value of a variable \\
    \hline UNARY\_OPERATOR & Insert unary operator \\
    \hline
  \end{tabular}
  \caption{The set of selective method-level mutation operators used in Javalanche.}
  \label{tab:javalanche_operators}
\end{table}

We chose Javalanche for our research because it is customizable and extensible, therefore allowing us to modify Javalanche to calculate unit mutation scores and output a richer set of results. Other benefits of Javalanche include (see Table~\ref{tab:mutation_tools} for all features): full integration with JUnit, the use of mutation schemas and bytecode generation to improve performance, and test selection using coverage. Although Javalanche does not have class-level mutation operators, due to the open source nature of Javalanche we plan to extend it to incorporate class-level mutation operators. In addition, Javalanche does have concurrency-level mutation operators as well as the ability to identify equivalent mutants using impact analysis.


\subsection{LIBSVM}
\label{subsec:approach_libsvm}
In our research we use LIBSVM (version 3.12), a \gls{svm} library capable of solving \emph{n}-group classification problems~\cite{CL11}. We decided to use this library implementation as it is mature and used in many other publications\footnote{LIBSVM~\cite{CL11} has been cited 9323 times according to Google Scholar as of May 21$^{st}$, 2012}. LIBSVM has the ability to run entirely from a \gls{cli}, and provides an easy to use interface to perform training and prediction.


\subsection{Eclipse Metrics Plugin}
\label{subsec:approach_metrics_plugin}
In our research, we use the Eclipse Metrics Plugin (version 1.3.8.20100730-001) to acquire source code metrics of the method- and class-level source code unit under test~\cite{Metrics}. We selected this tool as it provides a comprehensive set of metrics for Java programs (see feature sets \ding{172}~\&~\ding{174} from Table~\ref{tab:metrics}). The metrics can also be exported to \gls{xml} which is a suitable format to extract data from. Though this tool is part of Eclipse as a plugin it is possible to initiate the tool through a \gls{cli} interface after importing the \gls{sut} into Eclipse.


\subsection{EMMA}
\label{subsec:approach_emma}
We focus on JUnit test cases as our testing framework, thus can actually use the Eclipse Metrics Plugin to gather the source code metrics of the test suite (see feature set \ding{175} from Table~\ref{tab:metrics}). In order to gather other test suite coverage metrics we use EMMA (version 2.0.5312) which is capable of determining the basic block coverage of a test suite~\cite{EMMA}. Specifically, we use EMMA to acquire metrics for feature set \ding{173} from Table~\ref{tab:metrics}.


\section{Process}
\label{sec:approach_process}
Our process for mutation score prediction using source code and test suite metrics is shown in Figure~\ref{fig:process}. The complete set of source code and test suite metrics used in our process are shown in in Table~\ref{tab:metrics}. As we are using a supervised learning technique for prediction, a \gls{svm}, we need to initially collect training data. Thus we inevitably have to calculate the mutation scores of a software system at least once. As mentioned in the motivation (see Section~\ref{sec:introduction_motivation}) our approach aims to reduce the amount of mutation testing done in iterative development. If our technique generalize well then it can be possible to build a comprehensive model and predict on different software systems without any prior mutation testing (we explore this in Chapter~\ref{chap:experiment}).

\begin{figure}[h]
  \centering
  \includegraphics[width=11cm]{figures/process.pdf}
  \caption{The complete process for collecting data used to training a SVM to predict mutation scores.}
  \label{fig:process}
\end{figure}

As our approach attempts to predict the mutation score of source code units we need to keep in mind of the factors involved: the source code unit and any unit test cases that provides coverage. Our intuition suggests that we need to look at both source code and test suite metrics to properly measure these two source code artifacts. We hope that by considering the associated unit test cases for a source code unit we can capture a bit on their interactions and relationships in terms of test suite effectiveness. By using the tools described in Section~\ref{sec:approach_tools}, Eclipse Metrics Plugin and EMMA, we acquire the source code unit attributes described in Table~\ref{tab:metrics}. We aggregate method-level metrics into class-level metrics to follow the scope hierarchy. We also compute the mutation scores using Javalanche and combine those with the source code unit attributes to create our required input for training and prediction. The following sections walk through the complete process one phase at a time, providing examples where possible. The entire process is executed using a set of scripts that automates data collection, synthesis, and evaluation\footnote{Set of scripts are found at https://github.com/sqrg-uoit/mutation\_score\_predictor}.

\begin{table}[h]
  \centering
  \rowcolors{2}{gray!30}{gray!20}
  \begin{tabular}{|l|l|l|l|}
    \hline
    \rowcolor[RGB]{169,196,223}
    \textbf{Metrics} & \textbf{Description} & \textbf{Scope} & \textbf{Set} \\

    % Set 1: Source Code Metrics
    \hline MLOC & Method lines of code & Method & \ding{172} \\
    \hline NBD & Nested block depth & Method & \ding{172} \\
    \hline VG & McCabe cyclomatic complexity & Method & \ding{172} \\
    \hline PAR & Number of parameters & Method & \ding{172} \\
    \hline NORM & Number of overridden methods & Class & \ding{172} \\
    \hline NOF & Number of attributes & Class & \ding{172} \\
    \hline NSC & Number of children & Class & \ding{172} \\
    \hline DIT & Depth of inheritance tree & Class & \ding{172} \\
    \hline LCOM & Lack of cohesion of methods & Class & \ding{172} \\
    \hline NSM & Number of static methods & Class & \ding{172} \\
    \hline NOM & Number of methods & Class & \ding{172} \\
    \hline SIX & Specialization index & Class & \ding{172} \\
    \hline WMC & Weighted method per class & Class & \ding{172} \\
    \hline NSF & Number of static attributes & Class & \ding{172} \\

    % Set 2: Coverage Metrics
    \hline BCOV & Basic blocks covered in code unit & Class/Method & \ding{173} \\
    \hline BTOT & Total basic blocks for code unit & Class/Method & \ding{173} \\
    \hline NOT & Number of test cases & Class/Method & \ding{173} \\

    % Set 3: Accumulated Source Code Metrics
    \hline SMLOC & Sum MLOC of methods & Class & \ding{174} \\
    \hline SNBD & Sum NBD of methods & Class & \ding{174} \\
    \hline SVG & Sum VG of methods & Class & \ding{174} \\
    \hline SPAR & Sum PAR of methods & Class & \ding{174} \\
    \hline AMLOC & Average MLOC of methods & Class & \ding{174} \\
    \hline ANBD & Average NBD of methods & Class & \ding{174} \\
    \hline AVG & Average VG of methods & Class & \ding{174} \\
    \hline APAR & Average PAR of methods & Class & \ding{174} \\

    % Set 4: Accumulated Test Case Metrics
    \hline STMLOC & Sum MLOC of test methods & Class/Method & \ding{175} \\
    \hline STNBD & Sum NBD of test methods & Class/Method & \ding{175} \\
    \hline STVG & Sum VG of test methods & Class/Method & \ding{175} \\
    \hline STPAR & Sum PAR of test methods & Class/Method & \ding{175} \\
    \hline ATMLOC & Average MLOC of test methods & Class/Method & \ding{175} \\
    \hline ATNBD & Average NBD of test methods & Class/Method & \ding{175} \\
    \hline ATVG & Average VG of test methods & Class/Method & \ding{175} \\
    \hline ATPAR & Average PAR of test methods & Class/Method & \ding{175} \\
    \hline
  \end{tabular}
  \caption{The complete set of metrics used as attributes for each vector of the \gls{svm}.}
  \vspace{1mm}
  \footnotesize{\emph{Table~\ref{tab:metrics} shows the complete set of metrics used as attributes for each vector of the \gls{svm}, categorized by feature set (\ding{172}: Source Code Metrics, \ding{173}: Coverage Metrics, \ding{174}: Accumulated Source Code Metrics, \ding{175}: Accumulated Test Case Metrics).}}
  \vspace{1mm}
  \label{tab:metrics}
\end{table}


\subsection{Inputs}
\label{subsec:approach_inputs}
To predict the mutation score of source code units (methods and classes) our approach requires the following inputs: a set of source units of code (the Java files that compose the \gls{sut}) and the corresponding set of unit test cases (the JUnit files that compose the test suite for the \gls{sut}). A simple example of the input our approach requires is presented in Figure~\ref{fig:triangle_example}. Our approach is only concerned with source units of code that are being tested, thus the more coverage the test suite provides the more data that can be extracted from the \gls{sut}.

\begin{figure}[h]
  \centering
  \begin{minipage}{7.25cm}
  \centering
  \footnotesize{\textbf{\texttt{Triangle} Source Code}}
  \lstinputlisting[language=Java]{listings/Triangle.java}
  \end{minipage}
  \hfill
  \begin{minipage}{7.25cm}
  \centering
  \footnotesize{\textbf{\texttt{TriangleTest} Test Suite}}
  \lstinputlisting[language=Java]{listings/TriangleTest.java}
  \end{minipage}
  \caption{Example source code of \texttt{Triangle} and its test suite \texttt{TriangleTest}.}
  \vspace{1mm}
  \footnotesize{Figure~\ref{fig:triangle_example} presents a stripped down example of expected input that our prediction approach requires. This system is able to classify triangles, and has a few test cases to test its capabilities in classifying triangles.}
  \vspace{1mm}
  \label{fig:triangle_example}
\end{figure}


\subsection{Collect Mutation Scores}
\label{subsec:approach_collect_mutation_scores}
We use \gls{svm}, a supervised learning technique, to predict mutation scores. Before any predictions can occur we must first collect data to compose a feature set with vectors of attributes. The collected data must also have their correct categories assigned to them as we will use the collected data for training purposes. Afterwards when training is completed it becomes possible to make predictions on new data based on the model that has been trained.

Javalanche is the mutation testing tool that our approach uses. Using scripts we made our whole approach as automated as possible, thus the user only has to configure a couple variables to target a different project (i.e., package prefix, test suite name, test \& source directories). We have made minor modifications to Javalanche that allows it to use all the specified operators from Table~\ref{tab:javalanche_operators}. Javalanche also is configured to use its coverage impact analysis to give insight on equivalent mutants (more on this in Chapter~\ref{chap:experiment}) though this slows down Javalanche substantially. Furthermore we added a custom analyzer that outputs the mutation scores of each class and method units in the \gls{sut}.

Javalanche generates all possible mutants, then considers the set of them that are covered by the provided test suite. Given the set of covered mutants Javalanche then tests and records the results of each mutant using its subset of covered test cases for that specific mutant. The newly added analyzer for Javalanche then outputs an intermediate \gls{csv} file of mutation scores for the covered source code units. Using the example \texttt{Triangle} system presented in Section~\ref{subsec:approach_inputs} the \gls{csv} file of the acquire mutation scores are shown in Figure~\ref{fig:triangle_mutation_scores}. Using the \gls{csv} file we populate a database with all the acquired data, this eases management and analysis of the data.

\begin{figure}[h]
  \centering
  \lstinputlisting[literate={,}{\textbf{,}}{1}]{listings/Triangle_class_mutation_score.csv}
  \lstinputlisting[literate={,}{\textbf{,}}{1}]{listings/Triangle_method_mutation_score.csv}
  \caption{Example \gls{csv} files of the mutation scores from the \texttt{Triangle} system.}
  \vspace{1mm}
  \footnotesize{Figure~\ref{fig:triangle_mutation_scores} shows the class (top) and method (bottom) mutation score \gls{csv} files. There are more values related to the number of mutant types generated/killed that are not shown for terseness.}
  \vspace{1mm}
  \label{fig:triangle_mutation_scores}
\end{figure}


\subsection{Collect Test Suite Coverage Metrics}
\label{subsec:approach_collect_coverage_metrics}
We collect test suite coverage metrics (see feature set \ding{173} in Table~\ref{tab:metrics}) using EMMA. Using the test suite and the \gls{sut} it is possible to acquire the coverage for each source code unit using the set of covering unit test cases\footnote{Currently we acquire the covered test cases using Javalanche, though this can easily be computed solely using EMMA with additional analysis}. We run the set of covered unit test cases for each source code unit with EMMA, this produces \gls{xml} files containing the block coverage of of the covered unit test cases on the \gls{sut}. We then can extract the coverage of the targeted source code unit. Using the example in Figure~\ref{fig:triangle_example} this phase extracts the following metrics as seen in Table~\ref{tab:triangle_coverage_metrics}.

\begin{table}[h]
  \centering
  \rowcolors{2}{gray!30}{gray!20}
  \begin{tabular}{|l|r|r|r|}
    \hline
    \rowcolor[RGB]{169,196,223}
    \textbf{Source Code Unit} & \textbf{NOT} & \textbf{BCOV} & \textbf{BTOT} \\
    \hline \texttt{Triangle} & 6 & 60 & 102 \\
    \hline \texttt{Triangle.classify} & 3 & 36 & 72 \\
    \hline \texttt{Triangle.isValid} & 6 & 24 & 30 \\
    \hline
  \end{tabular}
  \caption{Extracted coverage test suite metrics of the \texttt{Triangle} system.}
  \vspace{1mm}
  \footnotesize{Table~\ref{tab:triangle_coverage_metrics} is showing the coverage test suite metrics of feature set \ding{173} of Table~\ref{tab:metrics}.}
  \vspace{1mm}
  \label{tab:triangle_coverage_metrics}
\end{table}


\subsection{Collect Source Code Metrics}
\label{subsec:approach_collect_source_metrics}
We use the Eclipse Metrics Plugin to analyze the software system to produce an \gls{xml} file of the source code metrics of the source code units and unit test cases. The produced \gls{xml} file is \emph{metric-oriented}, so we translate so it is \emph{unit-oriented}. This phase acquires source code metrics (see feature set \ding{172} in Figure~\ref{tab:metrics}) for each source code unit and unit test case. Using the example in Figure~\ref{fig:triangle_example} this phase extracts the following metrics as seen in Table~\ref{tab:triangle_class_extracted_metrics}~\&~\ref{tab:triangle_method_extracted_metrics}.

\begin{landscape}
  \begin{table}[h]
    \centering
    \rowcolors{2}{gray!30}{gray!20}
    \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|}
      \hline
      \rowcolor[RGB]{169,196,223}
      \textbf{Source Code Unit} & \textbf{NORM} & \textbf{NOF} & \textbf{NSC} & \textbf{DIT} & \textbf{LCOM} & \textbf{NSM} & \textbf{NOM} & \textbf{SIX} & \textbf{WMC} & \textbf{NSF} \\
      \hline \texttt{Triangle} & 0 & 0 & 0 & 1 & 0 & 0 & 2 & 0 & 20 & 0 \\
      \hline \texttt{TriangleTest} & 0 & 0 & 0 & 1 & 0 & 0 & 6 & 0 & 6 & 0 \\
      \hline
    \end{tabular}
    \caption{Extracted class source code metrics of the \texttt{Triangle} system.}
    \label{tab:triangle_class_extracted_metrics}
  \end{table}

  \begin{table}[h]
    \centering
    \rowcolors{2}{gray!30}{gray!20}
    \begin{tabular}{|l|r|r|r|r|}
      \hline
      \rowcolor[RGB]{169,196,223}
      \textbf{Source Code Unit} & \textbf{MLOC} & \textbf{NBD} & \textbf{VG} & \textbf{PAR} \\
      \hline \texttt{Triangle.classify} & 20 & 1 & 13 & 3 \\
      \hline \texttt{Triangle.isValid} & 5 & 1 & 7 & 3 \\
      \hline \texttt{TriangleTest.testScalene} & 2 & 1 & 1 & 0 \\
      \hline \texttt{TriangleTest.testIsoceles} & 2 & 1 & 1 & 0 \\
      \hline \texttt{TriangleTest.testEquiliteral} & 2 & 1 & 1 & 0 \\
      \hline \texttt{TriangleTest.testNegative} & 2 & 1 & 1 & 0 \\
      \hline \texttt{TriangleTest.testInvalid} & 2 & 1 & 1 & 0 \\
      \hline \texttt{TriangleTest.testValid} & 2 & 1 & 1 & 0 \\
      \hline
    \end{tabular}
    \caption{Extracted method source code metrics of the \texttt{Triangle} system.}
    \label{tab:triangle_method_extracted_metrics}
  \end{table}
\end{landscape}


\subsection{Combine Coverage and Source Metrics}
\label{subsec:approach_combine_metrics}
At this point in the process we have acquired all the raw data (mutation scores, source code metrics, and test suite metrics). We now begin synthesizing data together, in this phase we combine source code metrics and coverage metrics together. We first analyze all the coverage \gls{xml} files produced from the coverage phase (see Section~\ref{subsec:approach_collect_coverage_metrics}. We calculate the coverage that each source code unit has given the set of covered unit test cases used. Now we combine the source code metrics and coverage metrics of a source code unit. The combined data is put into our database to go along with the acquire mutation score. Each source code unit in the database eventually will contain all the metrics pertaining to it, along with its mutation score.


\subsection{Aggregate and Merge Method-Level Metrics}
\label{subsec:approach_aggregate_merge_metrics}
The last phase for data synthesis is to merge the source code metrics of the covered unit test cases together into their corresponding source code unit. This merger acquires feature set \ding{175} from Table~\ref{tab:metrics}. Using the example in Figure~\ref{fig:triangle_example} this phase merges the source code metrics of the unit test cases into the source code unit's metrics as seen in Table~\ref{tab:triangle_merge_test_metrics}.

We also aggregate the method-level source code units metrics into their respected parent class-level source code unit. This allows us to populate the database with metrics from feature set \ding{174} from Table~\ref{tab:metrics}. Using our example the aggregation of method-level metrics to class-level source code units is shown in Table~\ref{tab:triangle_aggregate_metrics}.

\begin{landscape}
  \begin{table}[h]
    \centering
    \rowcolors{2}{gray!30}{gray!20}
    \begin{tabular}{|l|r|r|r|r|r|r|r|r|}
      \hline
      \rowcolor[RGB]{169,196,223}
      \textbf{Source Code Unit} & \textbf{STMLOC} & \textbf{STNBD} & \textbf{STVG} & \textbf{STPAR} & \textbf{ATMLOC} & \textbf{ATNBD} & \textbf{ATVG} & \textbf{ATPAR}  \\
      \hline \texttt{Triangle} & 18 & 9 & 9 & 0 & 9 & 4.5 & 4.5 & 0 \\
      \hline \texttt{Triangle.classify} & 6 & 3 & 3 & 0 & 2 & 1 & 1 & 0 \\
      \hline \texttt{Triangle.isValid} & 12 & 6 & 6 & 0 & 2 & 1 & 1 & 0 \\
      \hline
    \end{tabular}
    \caption{Merged test suite metrics for each source code unit of the \texttt{Triangle} system.}
    \vspace{1mm}
    \footnotesize{Table~\ref{tab:triangle_merge_test_metrics} is showing the merged test suite metrics of feature set \ding{175} of Table~\ref{tab:metrics}.}
    \vspace{1mm}
    \label{tab:triangle_merge_test_metrics}
  \end{table}

  \begin{table}[h]
    \centering
    \rowcolors{2}{gray!30}{gray!20}
    \begin{tabular}{|l|r|r|r|r|r|r|r|r|}
      \hline
      \rowcolor[RGB]{169,196,223}
      \textbf{Source Code Unit} & \textbf{SMLOC} & \textbf{SNBD} & \textbf{SVG} & \textbf{SPAR} & \textbf{AMLOC} & \textbf{ANBD} & \textbf{AVG} & \textbf{APAR}  \\
      \hline \texttt{Triangle} & 37 & 8 & 26 & 6 & 18.5 & 4 & 13 & 3 \\
      \hline
    \end{tabular}
    \caption{Aggregation of method-level source code metrics for each class-level source code unit of the \texttt{Triangle} system.}
    \vspace{1mm}
    \footnotesize{Table~\ref{tab:triangle_aggregate_metrics} is showing the aggregation of method-level source code metrics of feature set \ding{174} of Table~\ref{tab:metrics}.}
    \vspace{1mm}
    \label{tab:triangle_aggregate_metrics}
  \end{table}
\end{landscape}


\subsection{Create LIBSVM File}
\label{subsec:approach_create_libsvm_file}
Finally at this point in the process our database contains all the necessary information for the \gls{svm}. We have collected and synthesized all the source code and test suite metrics for both class- and method-level source code units. We can now output the specific file format \emph{.libsvm} of the acquired data. This format is required for our \gls{svm} tool LIBSVM and contains a list of vectors with each having a category and set of attributes, as seen in Figure~\ref{fig:libsvm_file}. Our process produces two \emph{.libsvm} files, one for the method-level source code units and another for the class-level source code units.

\begin{figure}[h]
  \centering
  \begin{minipage}{9.5cm}
    \lstinputlisting{listings/libsvm_example.libsvm}
  \end{minipage}
  \caption{Example file format for LIBSVM, a \emph{.libsvm} file of vectors}
  \vspace{1mm}
  \footnotesize{Figure~\ref{fig:libsvm_file} shows an example \emph{.libsvm} file required for LIBSVM. In \gls{svm} terms: each row is a vector, the first number in each row is the category and each \texttt{<a>:<b>} represent an attribute. From the example we can see that there are 3 categories (1, 2, and 3) and 12 attributes (1--12). For each attribute \texttt{a} represents the attribute ID and \texttt{b} represents the actual value for that attribute. The attribute ID maps to a specify metrics that the vector is representing. For example, attribute \text{1} might map to the \texttt{MLOC} attribute, and so-forth.}
  \vspace{1mm}
  \label{fig:libsvm_file}
\end{figure}


\subsection{Create LIBSVM File}
\label{subsec:approach_create_libsvm_file}
Next, we create a .libsvm file containing the category and feature data from the database. Instead of predicting a specific mutation score percentage, we categorize all mutation scores as \textit{low, medium, high} which reduces the mutation score prediction to a three-group classification problem. The ranges of values in each category are determined based on the distribution of the mutation scores in our training data (further explained in Section~\ref{sec:experiment_results}). Finally, the .libsvm file is passed into LIBSVM to complete the training process.

Mutation scores have a range between 0.0 and 1.0, unfortunately a \gls{svm} cannot perform classification over such a range of real numbers. To circumvent this problem we instead group ranges of mutation scores into groups (i.e., low: 0.00--0.33, medium: 0.34--0.66, and high: 0.67--1.00). Chapter~\ref{chap:experiment} explores ranges of mutation scores for our categories.


\section{Prediction}
\label{sec:approach_prediction}
Once we have trained the \gls{svm}, we can then use the \gls{svm} for prediction. We can predict the mutation score category of an unknown source code unit by first determining the source code and test suite metrics. The metrics (i.e., attributes) are passed into the \gls{svm} which will then assign a category of \textit{low, medium, high} for the mutation score.


\section{Related Work}
\label{sec:approach_related_work}
The use of software metrics to locate faults in source code has been well researched. For example, Koru et al. utilized static software measure along with defect data at the class level to predict bugs using machine learning~\cite{KL05}. Similarly, Gyimothy et al. used object-oriented metrics with logistic regression and machine learning techniques to identify faulty classes in open source software~\cite{GFS05}. Finally, design level metrics were used with a linear prediction model to determine the estimated maintainability and error prone modules of large software systems~\cite{MKPS00}. Our work is unique in comparison to these previous works since we not only use source code metrics but we also use test suite metrics to enhance our predication capabilities.


\section{Summary}
\label{sec:approach_summary}
This chapter we cover all aspects of our approach in terms of tools and steps used to collect and train our \gls{svm}. Using the tools described in Section~\ref{sec:approach_tools} we are able to collect source code and test suite metrics of source code units. As we use a \gls{svm} as our prediction technique we require training data, thus we also collect the mutation scores of each source code unit. Synthesizing all the acquire data allows us to train our \gls{svm} to make prediction on existing and new data. We described each step of our process in Section~\ref{sec:approach_process}, and how we perform prediction in Section~\ref{sec:approach_prediction}. Finally in Section~\ref{sec:approach_related_work} we address related work to our approach on prediction of mutation scores using machine learning prediction techniques.
