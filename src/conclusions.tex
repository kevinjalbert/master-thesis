% !TEX root = thesis.tex
\chapter{Summary and Conclusions}
\label{chap:conclusions}
We present a complete summary of the thesis in Section~\ref{sec:conclusions_summary}. We indicate the contributions this thesis has accomplished in Section~\ref{sec:conclusions_contributions}. In Section~\ref{sec:conclusions_limitations} we raise a number of concerns and limitations as well as how we mitigated these where possible. We outline alternative steps in our approach and experiments as well as additional future work in Section~\ref{sec:conclusions_future_work}. Finally we conclude this thesis in Section~\ref{sec:conclusions_conclusions}.


\section{Summary}
\label{sec:conclusions_summary}
Mutation testing is a resource intensive process as there is potentially thousands of mutants (i.e., versions of \gls{sut} with a single syntactic change that might induce a fault) that can be produced for each \gls{sut}. Mutation testing generates a set of mutants from the source code of the \gls{sut} and than evaluates these using the provided test suite. A mutation score is calculated as a result of mutation testing, which in indicates how effective the test suite is at finding faults (i.e., test suite effectiveness).

Oppose to an improving mutation testing performance by adjusting the mutation testing process (i.e., better mutation representation/generation/evaluation) we decided to apply machine learning to make predictions for the mutation score of source code units. We consider the source code and test suite as strong sources of features/attributes for our predictions as they are directly involved in the mutation testing process. As described in Chapter~\ref{chap:approach} we use a \gls{svm} to make predictions based on the features of class- and method-level source code units. We identify 4 initial sets of metrics (i.e., feature sets) from the \gls{sut}: source code, coverage, accumulated source code and accumulated test suite.

We outline our 8 experimental subjects that contained about 1689 classes, 113686 method and 10233 test cases in Section~\ref{sec:experiment_subjects}, in which the total time required for mutation tested was about 64.5 hours. Our approach was able to collect 864 class-level and 5510 method-level source code units that were covered by mutation testing and fit our criteria for training and prediction. We observed the mutation score distribution of all covered source code units in Section~\ref{subsec:experiment_mutation_score_distribution} and determined a three category approach to abstract the real-values of mutation scores.

In Section~\ref{subsec:experiment_cross_validation} we used our selected categories we performed cross-validation utilizing all available data with undersampling over the different feature sets. Our results showed that the combination of the feature sets provides a higher cross-validation accuracy. We then observed the prediction accuracy on unknown data in Section~\ref{subsec:experiment_prediction} using the individual subjects and sets of all but a single subject. Our results here showed that class-level mutation score prediction was either harder to predict or missing additional features. Method-level predictions were better than class-level predictions, furthermore training and predicting contained to an individual subject yielded higher accuracy than that of the all but a single subject. Due to limited data the individual prediction accuracy had large variations in prediction accuracy. We identified drawbacks to our prediction experiment in Section~\ref{subsec:experiment_optimization_generalization} that showed using cross-validation accuracy for \gls{svm} parameter selection can be ineffective. We presented several alternative and more effective performance measures for machine learning techniques, namely \emph{F-score}. Using F-score we conduced our own grid search for a single set of \gls{svm} parameters that maximized F-score across all predictions. We identified generalizable \gls{svm} parameters for class- and method-level predictions, and redid our prediction accuracy experiment using the new parameters. We were able to increase the average prediction accuracy by 4.8147 and 3.7847 for class- and method-level source code units respectively. We looked at the implications of data availability and potential usage for iterative development in Section~\ref{subsec:experiment_data}. By limiting the amount of data used for training we were able to find that roughly \sfrac{1}{3} of the data is needed for training to maximize prediction accuracy. 


\section{Contributions}
\label{sec:conclusions_contributions}
Throughout the experiment chapter (see Chapter~\ref{chap:experiment} we noted the following in terms of contributes to the domain of mutation score prediction:

\begin{itemize}
	\item Identified several features that can be used to prediction mutation scores of source code units
	\item Presented the mutation score distribution of covered mutants over 8 open-source systems
	\item Observed the effect on cross-validation accuracy with respect to the feature sets
	\item Observed the effect of predicting on unknown source code units (i.e., not used for training) both within an individual project and across projects
	\item Found a set of \gls{svm} parameters for class- and method-level predictions that maximize generalizability in the observed open-source systems.
	\item Understood the impact on prediction accuracy based on the amount of data from an individual project is used for training.
\end{itemize}

Specifically we showed that using all the available features we were able to achieve an average prediction accuracy of 37.8498\% and 49.7920\%, for class- and method-level source code units respectively. Method-level predictions have a much higher accuracy than class-level predictions, which is alright as method-level source code units are finer grain in terms of scope. We achieved higher than random prediction accuracies using generalized \gls{svm} parameters, which removes the need of finding suitable parameters for new data. Our approach is novel in that we consider both source code and test suite metrics as factors in making the predictions in software testing. We also performed feature selection on the used features in Appendix~\ref{app:feature_selection}, which showed that it is possible to reduce feature set while retaining prediction accuracy.


\section{Limitations}
\label{sec:conclusions_limitations}


\section{Future Work}
\label{sec:conclusions_future_work}


\section{Conclusions}
\label{sec:conclusions_conclusions}
Our technique for predicting mutation score using source code and test suite metrics outperforms random with an achieved accuracy of 58.27\% and 54.82\% with the JGAP data, for classes and methods respectively. These results have not been optimized and we believe that with further enhancements and a more tailored feature set we may be able to increase the prediction accuracy.

Despite the promising initial results there is an obvious threat to external validity since we have applied our predictive technique to a single open source project -- JGAP. Additionally, we performed training and prediction from the same project. As stated by Kitchenham and Mendes \textit{``It is invalid to select one or two datasets to `prove' the validity of a new technique because we cannot be sure that, of the many published datasets, those chosen are the only ones that favour the new technique''}~\cite{KM09}. Thus, we plan to evaluate more open source projects using our prediction technique to better assess the prediction accuracy. With more data we plan to investigate whether cross-project models are valid for mutation score prediction. We would also like to consider projects with more varied mutation scores to explore the variation in prediction accuracy between strong and weak test suites. A final area of future work is to expand the set of mutation operators used to include object oriented and concurrency operators.
