% !TEX root = thesis.tex
\chapter{Summary and Conclusions}
\label{chap:conclusions}
We present a complete summary of the thesis in Section~\ref{sec:conclusions_summary}. We indicate the contributions this thesis has accomplished in Section~\ref{sec:conclusions_contributions}. In Section~\ref{sec:conclusions_limitations} we raise a number of concerns and limitations as well as how we mitigated these where possible. We outline alternative steps in our approach and experiments as well as additional future work in Section~\ref{sec:conclusions_future_work}. Finally we conclude this thesis in Section~\ref{sec:conclusions_conclusions}.


\section{Summary}
\label{sec:conclusions_summary}
Mutation testing is a resource intensive process as there is potentially thousands of mutants (i.e., versions of \gls{sut} with a single syntactic change that might induce a fault) that can be produced for each \gls{sut}. Mutation testing generates a set of mutants from the source code of the \gls{sut} and than evaluates these using the provided test suite. A mutation score is calculated as a result of mutation testing, which in indicates how effective the test suite is at finding faults (i.e., test suite effectiveness).

Oppose to an improving mutation testing performance by adjusting the mutation testing process (i.e., better mutation representation/generation/evaluation) we decided to apply machine learning to make predictions for the mutation score of source code units. We consider the source code and test suite as strong sources of features/attributes for our predictions as they are directly involved in the mutation testing process. As described in Chapter~\ref{chap:approach} we use a \gls{svm} to make predictions based on the features of class- and method-level source code units. We identify 4 initial sets of metrics (i.e., feature sets) from the \gls{sut}: source code, coverage, accumulated source code and accumulated test suite.

We outline our 8 experimental subjects that contained about 1689 classes, 113686 method and 10233 test cases in Section~\ref{sec:experiment_subjects}, in which the total time required for mutation tested was about 64.5 hours. Our approach was able to collect 864 class-level and 5510 method-level source code units that were covered by mutation testing and fit our criteria for training and prediction. We observed the mutation score distribution of all covered source code units in Section~\ref{subsec:experiment_mutation_score_distribution} and determined a three category approach to abstract the real-values of mutation scores.

In Section~\ref{subsec:experiment_cross_validation} we used our selected categories we performed cross-validation utilizing all available data with undersampling over the different feature sets. Our results showed that the combination of the feature sets provides a higher cross-validation accuracy. We then observed the prediction accuracy on unknown data in Section~\ref{subsec:experiment_prediction} using the individual subjects and sets of all but a single subject. Our results here showed that class-level mutation score prediction was either harder to predict or missing additional features. Method-level predictions were better than class-level predictions, furthermore training and predicting contained to an individual subject yielded higher accuracy than that of the all but a single subject. Due to limited data the individual prediction accuracy had large variations in prediction accuracy. We identified drawbacks to our prediction experiment in Section~\ref{subsec:experiment_optimization_generalization} that showed using cross-validation accuracy for \gls{svm} parameter selection can be ineffective. We presented several alternative and more effective performance measures for machine learning techniques, namely \emph{F-score}. Using F-score we conduced our own grid search for a single set of \gls{svm} parameters that maximized F-score across all predictions. We identified generalizable \gls{svm} parameters for class- and method-level predictions, and redid our prediction accuracy experiment using the new parameters. We were able to increase the average prediction accuracy by 4.8147 and 3.7847 for class- and method-level source code units respectively. We looked at the implications of data availability and potential usage for iterative development in Section~\ref{subsec:experiment_data}. By limiting the amount of data used for training we were able to find that roughly \sfrac{1}{3} of the data is needed for training to maximize prediction accuracy. 


\section{Contributions}
\label{sec:conclusions_contributions}
Throughout the experiment chapter (see Chapter~\ref{chap:experiment} we noted the following in terms of contributes to the domain of mutation score prediction:

\begin{itemize}
	\item Identified several features that can be used to prediction mutation scores of source code units
	\item Presented the mutation score distribution of covered mutants over 8 open-source systems
	\item Observed the effect on cross-validation accuracy with respect to the feature sets
	\item Observed the effect of predicting on unknown source code units (i.e., not used for training) both within an individual project and across projects
	\item Found a set of \gls{svm} parameters for class- and method-level predictions that maximize generalizability in the observed open-source systems.
	\item Understood the impact on prediction accuracy based on the amount of data from an individual project is used for training.
\end{itemize}

Specifically we showed that using all the available features we were able to achieve an average prediction accuracy of 37.8498\% and 49.7920\%, for class- and method-level source code units respectively. Method-level predictions have a much higher accuracy than class-level predictions, which is all right as method-level source code units are finer grain in terms of scope. We achieved higher than random prediction accuracies using generalized \gls{svm} parameters, which removes the need of finding suitable parameters for new data. Our approach is novel in that we consider both source code and test suite metrics as factors in making the predictions in software testing. We also performed feature selection on the used features in Appendix~\ref{app:feature_selection}, which showed that it is possible to reduce feature set while retaining prediction accuracy.


\section{Limitations \& Threats to Validity}
\label{sec:conclusions_limitations}
As with any study/experiment there will be limitations and threats to validity. In Section~\ref{subset:conclusions_limitations_approach} we discuss limitations with our approach. We consider the four categories for threats to validity with respect to experimentation in Section~\ref{subsec:conclusions_conclusion_validity}--~\ref{subsec:conclusions_external_validity}~\cite{WRH+00,WKP10}.


\subsection{Limitations of Approach}
\label{subset:conclusions_limitations_approach}
Our approach for predicting mutation scores based on source code and test suite metrics utilizes a number of tools. Namely we have: \emph{Javalanche} to collect mutation scores, \emph{Eclipse Metrics Plugin} to collect source code and test suite metrics, \emph{EMMA} to collect additional test suite coverage metrics, and \emph{LIBSVM} to perform the training and prediction of the source code units. We selected tools based on the metrics they could provide as well as the the output format, there might be other candidates tool that could have performed better. In particular the mutation testing tool we selected is not the newest, and omits a whole class of mutations (i.e., class-level object-oriented mutants), which could be misrepresenting the mutation scores. In our approach we had removed any source code unit that was abstract/overloaded/anonymous due to their complexity, which reduced our usable data and could have misrepresented the actual systems. The tools used to collect the features of the source code units might not be comprehensive in terms of features that describe the source code units. 


\subsection{Conclusion Validity}
\label{subsec:conclusions_conclusion_validity}
Threats to conclusion validity involve issues with the process and statistical means to draw any conclusions regarding experiments. We utilized various summary statistical measures to determine the conclusions of our results. In particular we used the following statistical measures: mean, standard deviation, quartiles and frequencies to understand our experiments with respect to their results. Furthermore with our results we conducted a minimum of 10 executions per experiment to mitigate the randomness of our results. With respect to drawing conclusions were comparing the average accuracy to what a random prediction would achieve. Thus by comparing the mean accuracies we were able to compare our approach to random. In retrospect we should have performed more executions per experiment to further reduce the noise. Furthermore we could have performed a statistical test to understand the statistical significants of our comparison.


\subsection{Internal Validity}
\label{subsec:conclusions_internal_validity}
Internal threats to validity are concerned with factors that could influence the independent variable in our experiments. Our independent variables are the features themselves from the 8 open-source systems that we selected. Obviously there could be issues that can arise based on the measures that our tools returned for each system, though these tools are well established and provide simplistic measures (i.e., issues are unlikely to arise due to \emph{incorrect} results). With respect to the mutants themselves that are generated by the \emph{Javalanche} mutation testing tool, the version used was experimental and could be more susceptible to \emph{incorrect} results. Furthermore \emph{Javalanche} uses a subset of mutants (i.e., mainly method-level mutants), which could have a major impact on the class-level source code units. With respect to \emph{true} internal validity the independent variables are not influencing each other in ways that we were not aware of that could be detrimental to our experiment. 


\subsection{Construct Validity}
\label{subsec:conclusions_construct_validity}
Whether the independent and dependant variables we are using actually align with the problem we are experimenting with is an issue with construct validity. In our experiment we are using a set of features extracted from open-source software systems (i.e., the independent variables) to determine the accuracy of predicting mutation score (i.e, the dependant variable). Machine learning performance measures (i.e., accuracy, F-score, etc...) are valid dependant variables as they measure the effectiveness of the classification technique. The independent variables for machine learning are harder to determine by nature, there is often no clear set of features for making predictions. For our experiment we observed the two main components involved in mutation testing, and these are the source code and test suite. These two components can be represented in quantifiable metrics (i.e., source code and test suite metrics) which are commonly known and used in Software Engineering research. 


\subsection{External Validity}
\label{subsec:conclusions_external_validity}
With experiments one of the major concerns involve the ability for the results to generalize outside of the study, that is external validity. With our experiment we specifically avoided toy-problems and opted to use open-source systems, which are real software systems. These systems are not industrial system nor extremely large-scale (i.e., 100K+ \gls{sloc}), thus we are unsure if the results would generalize to such systems. The open-source systems we chose had some variation in domain (i.e., library, framework, etc...) though our set obviously does not act as a representative of different domains. In addition most of the subjects we used had relatively \emph{good} test suites (i.e., of the covered mutants the mutation scores were above 50\%), we are unsure how our prediction would perform on systems with \emph{poor} test suites. Furthermore we observed only the Java language, whether these results generalize to other languages has not been verified. As stated by Kitchenham and Mendes \textit{``It is invalid to select one or two datasets to `prove' the validity of a new technique because we cannot be sure that, of the many published datasets, those chosen are the only ones that favour the new technique''}~\cite{KM09}. We used only 8 open-source projects as our datasets for our prediction technique, even though this is more then one or two it is still quite limited.  Mutants can be influenced by external factors such test suite size and mutation operators as it was found that class-level mutants are harder to detect than traditional method-level mutants~\cite{NK11}. As we used only traditional mutation operators this could have an impact on the generalizability, same with the varying sizes of the test suites of our subjects.


\section{Future Work}
\label{sec:conclusions_future_work}
The future work for this thesis can be broken up into 3 different directions. First in Section~\ref{subsec:conclusions_optimizing_approach} we consider future work that can be done to imporove and optimize our approach for predicting mutation scores. Secondly in Section~\ref{subsec:conclusions_experimental_evaluation} we look at future work that can be done on the experimental and statistical side of this thesis. Finally we consider alternatives and future work to validate generalizability of our results.


\subsection{Optimizing and Improving Approach}
\label{subsec:conclusions_optimizing_approach}
In our approach we discard abstract/overloaded/anonymous source code units as they were a bit more complex to handle during the construction of our approach. As for future work it would be wise to reconsider these omitted details as they obviously contribute to the data (i.e., mainly for class-level source code units, which might explain why their prediction accuracy was lower). We would like to correctly determine the \emph{not} (i.e., number of tests) feature without relying on \emph{Javalanche} for coverage data. \emph{EMMA} is fully capable in determining the \emph{not} feature. On the topic of features we would like to further explore additional metrics and other facets of a software system. Specifically we could explore the mutants themselves as the mutation generation is not the expensive aspect of mutation testing, or even runtime information.

By using \emph{Javalanche} we unfortunately did not have access to class-level object-oriented mutation operators, and a limited subset of traditional method-level mutation operators. Future work would be required to add these missing mutation operators into \emph{Javalanche} as it would not only benefit this thesis yet those that use the tool. Furthermore there exists more than the traditional mutation operators that generate typical faults, it would be a great addition to incorporate security and concurrency mutation operators into our approach. Equivalent mutants pose a challenge in interpreting mutation testing results, \emph{Javalanche} has an approach that attempts to mitigate the impact of equivalent mutants that we ignored. Further work can integrate this consideration into our approach, we initially did not include it as it would further reduce our available data.

There are standard optimizations that can be done for our implementation such as better data structures and taking advantage of concurrency. We also would like to adapt our approach so that others can use it from a usability point-of-view. For example, a simple script that allows a user to specify source code unit(s) to be predicted based on a already trained classification model or as an Eclipse plugin. As our approach uses a classification approach for prediction, it is possible to extract from the \gls{svm} the probability that a vector belongs to a specific category. We would like to take advantage of this and present this data as well as it illustrates the confidence in the predictions.


\subsection{Statistical and Experimental Evaluation}
\label{subsec:conclusions_experimental_evaluation}
With our experimental setup we utilize a minimum of 10 executions to reduce the noise in our results. To further reinforce our results future work would involve increasing the number of executions (i.e., between 25 and 100 executions). In our analysis of the results we used primarily summary statistics (i.e., mean, standard deviation, etc...), while these statistics provide valid summary of the results there are other statistical measures that are stronger (i.e., confidence interval, hypothesis testing, etc...). Furthermore we performed a number of experiments to evaluate our approach, these alone are not comprehensive in what could have been done. Additional analysis on the features and their relationship with each other and mutation scores would be an interesting study to conduct as it may provide additional detail on the source code units. For future work we would like to evaluate our current results and new experiments with stronger statistical measures.


\subsection{Alternative and Generalizability of Findings}
\label{subsec:conclusions_generalizability_findings}
With respect to our implementation we utilized a number of tools to gather features and the mutation scores. We would like to explore using other tools as alternatives, this has two effects: (1) can allow a better tool to be used and (2) can show that our approach still functions correctly independently of the tools used. Due to our limited set of experimental subjects we did not have a wide variety of domains, source sizes, test suite sizes and mutation scores. By including more open-source and potentially industrial software systems we can cover more pairings of the aforementioned criteria, which will shine insight on the generalizability of our approach. 


\section{Conclusions}
\label{sec:conclusions_conclusions}
We presented a problem in Chapter~\ref{chap:introduction} on how mutation testing is too costly. With the thesis statement we set out to create an approach that could predict mutation scores based on source code and test suite metrics using machine learning. The necessary topics required to solve the thesis statement were covered with detail in Chapter~\ref{chap:background}. Using the presented background knowledge we described out approach along-side an example in Chapter~\ref{chap:approach}. We outlined a set of experiments in Chapter~\ref{chap:experiment} that we conducted and discussed the results. Finally we present limitations, threats to validity and future work in Chapter~\ref{chap:conclusions}.

With our approach we showed that it is indeed possible to predict mutation scores of source code units using source code and test suite metrics. We were able to achieve an average of 37.8498\% and 49.7920\% prediction accuracy for class- and method-level source code units respectively. Both of these values are higher than random prediction accuracy (i.e., 33.3333\%) using a general set of \gls{svm} parameters, which eases the complexity of tuning our technique. Contrary to other prediction techniques (i.e., bug detection) we observed the test suite in addition to the source code, which is quite novel. With future work we hope that test suite metrics can be further used in existing and future research. Furthermore we anticipate that our approach still has room for improvement with respect to generalizability and prediction accuracy.
